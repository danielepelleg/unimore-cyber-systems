# 5. Semantic Text Analysis

## Accesso ai Dati

L’**Information Retrieval** è un insieme di tecniche che riguarda lo studio della *ricerca di dati*. Il **search engine** è il *principale strumento utilizzato per accedere ai dati*: utilizzandolo l'utente è in grado di cercare, mediante dell *keyword*, delle informazioni per lui interessanti. La **tecnica** che *sceglie i documenti interessanti rispetto alle keyword in input* prende il nome di **text retrieval**.

Generalmente, ci sono due modi per ottenere le informazioni:

- **`PULL`** : l'utente interroga il sistema mediante query in cui utilizza delle *keywords*
- **`PUSH`** : il sistema fornisce risultati considerati interessanti per l'utente utilizzando dei *sistemi di raccomandazione*

## Text Retrieval

Il **Text Retrieval** (*recupero del testo*) è la ***disciplina*** che si occupa di studiare i metodi per ***trovare le informazioni di interesse per l'utente***: utilizza le ***query utente*** per *identificare il **sottoinsieme di documenti che soddisfa la richiesta*** informativa dell'utente. Ha come obiettivo quello di restituire all'utente informazioni testuali corrette nel minor tempo possibile. Il Text Retrieval viene normalmente utilizzato all'interno dei *search engine*. La **difficoltà** dipende dalla *quantità di dati a disposizione*.

### Differenze tra Text Retrieval e Database Retrieval

- **`DATI`**: nei **database** i dati sono *strutturati tramite uno schema*. Le query sono diverse: nei database sono ben specificate utilizzando una determinata sintassi, mentre in un ***search engine*** non si utilizza un linguaggio formale, le query sono corte, il riconoscimento avviene mediante delle *parole chiave*, pertanto un motore di ricerca testuale risulta più complesso. 
- **`SISTEMA DI VALUTAZIONE`**: i *database vengono valutati in base all’efficienza*, cioè *differiscono per i tempi*, ma *la qualità del risultato deve essere la stessa* per tutti i `DBMS` a parità di query `SQL` (cosa che non si ha nel search engine). L'unico in grado di **valutare se un motore di ricerca funziona** correttamente è l'***utente***, la cui *opinione è soggettiva*. 
- **`OUTPUT`**: per un database consiste in ***una o più tuple***, mentre nel text retrieval vengono ritornati dei ***documenti***.

### Ricerca Mediante un Search Engine

1. Si suppone di avere un **vocabolario $V$** contenente tutte le *parole presenti nei documenti*:
   $$
   V = \{w_1,w_2,...,w_n\}
   $$

2. La **query $q$** dell'utente è una *sequenza di $m$ parole appartenenti al vocabolario*:
   $$
   q = \{q_1,q_2,...,q_m\} \quad \text{dove} \quad q_i \in V
   $$

3. Un **documento $d_i$** può essere visto come un insieme di $s$ parole appartenenti al vocabolario:
   $$
   d_i = \{d_{i1},...,d_{is}\} \quad \text{dove} \quad d_{ij} \in V
   $$

4. Una **collezione $C$** è un *insieme di documenti*:
   $$
   C = \{d_1,...,d_M\}
   $$

5. Si assume che esista un **sottoinsieme di documenti $R$ nella collezione che sono rilevanti rispetto alla query $q$**:
   $$
   \exists \text{ } R(q) \in C
   $$

6. L’**insieme dei documenti utili per l’utente è $R'(q)$**, ovvero un'approssimazione di $R(q)$: due utenti diversi infatti, facendo la stessa query, possono avere risultati diversi.

#### Document Selection vs. Document Ranking

Esistono principalmente due strategie per trovare il sottoinsieme di documenti rilevanti $R'(q)$:

1. **`DOCUMENT SELECTION`**: si implementa un ***classificatore binario*** che ***decide i documenti rilevanti*** o meno in relazione alla query. Viene fatta una *valutazione assoluta*: un documento è utile o non è utile. È **poco accurato**: è ***difficile decidere in maniera netta cosa è rilevante o meno per l'utente***.
   $$
   R'(q) = f(q,d)= \begin{cases} 0 \\ 1 \end{cases}
   $$
   
2. **`DOCUMENT RANKING`**: si fa un ***ranking dei documenti*** e ***si fa decidere all’utente il cutoff***. I documenti vengono ordinati in base alla loro importanza, l'utente può scorrerli e fermarsi in base alle proprie esigenze. Si tratta della ***strategia migliore***: è *più semplice da implementare* che generalmente *viene preferita*. Il problema consiste nello scrivere la **funzione di rilevanza** che ***deve definire uno score basandosi sulla query e sul documento***. È necessario definire la *rilevanza di un documento*.

### Modelli Retrieval

Per **implementare il Document Ranking** è necessaria una **funzione di rilevanza** che ***definisca uno score*** *in base ad una query e ad un documento, e che effettua un ranking in ordine decrescente di rilevanza*. Per avere una **definizione** computazionale **di *rilevanza*** è possibile utilizzare principalmente due *tipologie di modelli*:

1. **`MODELLI BASATI SULLA SIMILARITÀ`**: si effettua il ranking basandosi sul fatto che se un documento è più simile alla query rispetto ad un altro allora è più importante (es. *Vector Space Model*).
2. **`MODELLI PROBABILISTICI`**: si basa sul *concetto di probabilità*. Si valuta la ***probabilità che una query $R$ (variabile casuale) sia uguale a $1$*** per un documento. Così facendo il documento soddisfa l'esigenza informativa della query.

La maggior parte dei modelli per la *rappresentazione* utilizza le **`bag-of-words`**: un *gruppo di parole che sintetizza un documento*. 

> **`BAG-OF-WORDS`**
>
> Si costruisce una **matrice** avente come **colonne** le ***parole utilizzate nei diversi documenti***, mentre come **righe** i ***documenti***. L'elemento in posizione $(i,j)$ della matrice può presentare può presentare un $1$ o uno $0$ a seconda se il documento presenta o meno la parola, oppure può presentare il *conteggio di quella parola*, se presente nel testo. 

Il **problema** dell'utilizzo delle *bag-of-words* è che non considera l'*ordine dei termini nella query*. La funzione $f$ che si implementa permette però di considerare diversi parametri:

- **`term frequency`** - *occorrenza di una parola in un documento*
- **`document length`** - lunghezza del documento, *premia i documenti corti che contengono la parola* e *penalizza quelli lunghi*, in quanto la probabilità che questi contengano un determinato termine è maggiore
- **`document frequency`** - frequenza del termine in tutti i documenti, *premia i documenti che presentano termini rari*

#### Modelli Basati sulla Similarità

##### Vector Space Model - `VSM`

È un **modello semplice** per *costruire delle funzioni di ranking*, ***basato sulla similarità tra documento e query***. Non è una rappresentazione ottimale ma è sufficiente per molti problemi di ricerca. Utilizza il modello ***bag-of-word*** per dare una **rappresentazione vettoriale** sia del ***documento*** che della ***query*** in uno spazio di $N$ elementi (*termini dei documenti*). 

Per calcolare la **similarità** si utilizza la ***distanza del coseno***: tanto più il *vettore della query è vicino al vettore del documento, tanto più sono simili* e quindi il documento può essere considerato come risposta esatta. `VSM` non dice come definire la similarità o come posizionare i documenti nello spazio vettoriale.

La **versione più semplice** `VSM` viene così implementata:
$$
q = (x_1,...,x_n)
\qquad
d = (y_1,...,y_n)
\\\\
\text{dove }  x_i,y_i \in \{0,1\} \text{ a seconda della presenza della parola } W_i 
$$

$$
Sim(q,d) = q \cdot d = \sum^N_{i=1} x_iy_i = x_1y_1+...+x_Ny_N
$$

Questa versione *non considera la frequenza di un termine* nel documento ma solo la presenza, e *non considera l’importanza di una parola* rispetto ad un’altra. In una **versione migliorata** si tiene conto anche della ***Term Frequency***: se tra due documenti con lo stesso numero di keyword uno ne ha qualcuna con una frequenza maggiore, allora avrà un’importanza maggiore.

###### Inverse Document Frequency

L'**Inverse Document Frequency** è una *funzione* che viene ***utilizzata come soluzione al problema delle parole poco informative***: normalmente nel `VSM` parole diverse assumono la stessa importanza. Utilizzando **`IDF`** si *tiene conto della **`document frequency`***, ovvero della *frequenza dei termini in tutta la collezione*. Il **peso** (***importanza***) dei termini *frequenti* viene *diminuito*, mentre quello dei termini *poco frequenti aumentato*, dato il maggior contenuto informativo.

$$
IDF(w) = log(\frac{M+1}{k})
$$

- $M + 1$ : *numero di documenti della collezione* $+1$ (per evitare $log(1)=0$).
- $k$ : numero di documenti contenenti la parola $w$.

Viene considerato il **logaritmo** per *limitare* il valore. Il valore dato da $IDF(w)$ viene moltiplicato a $y_i$, ovvero la *frequenza dei termini relativa ad un documento*, in modo da pesarne meglio i termini. 

Rimane tuttavia il **problema della `term frequency`**, che fa alzare molto lo score del documento. Per ***calmierare il peso di una parola*** in un documento e risolvere il problema sono possibili due approcci:

- **`logaritmo`** : $y = \log(1+x)$ oppure $y=(1+\log(1+x))$
- **`trasformazione BM25`** : utilizza un termine $k$ per *limitare l'importanza di una parola* &rarr; $y=\frac{(k+1)x}{x+k}$

###### Normalizzare la Lunghezza del Documento

Viene **valutata la lunghezza dei documenti**: un documento lungo ha più probabilità di matchare con una query, dato che contiene più parole. Un documento più piccolo che contiene le parole della query è invece più facile che sia focalizzato sull'argomento in esame. Per **normalizzare la lunghezza del documento** si usa la *Pivoted Length Normalization*, che ***considera la lunghezza media dei documenti*** e imposta un parametro $b \in [0,1]$ per controllarne la normalizzazione.

Oltre a queste modifiche, si possono prendere altri accorgimenti come lo ***stemming***, che *considera le parole nella forma base* (senza declinazioni), ***rimuovere le stop word*** o utilizzare ***diverse funzioni di similarità***.

#### Modelli Probabilistici

La funzione di ranking si basa sulla probabilità che un documento $d$ abbia ***rilevanza*** $R=1$ per una query $q$:

$$
P(R=1 | d,q)= \frac{count(q,d,R=1)}{count(q,d)} \qquad \text{dove } R\in \{0,1\}
$$

È possibile usare una **tabella di rilevanza** in cui si segna, per ogni query e documento la rilevanza. In seguito si calcola la ***probabilità*** utilizzando la formula, come rapporto tra il numero di volte in cui la *query è stata rilevante per il documento*, e *il numero di presenze nella tabella*. Il **problema** però è che questo ***modello probabilistico non è scalabile*** nella costruzione della tabella, è impensabile ***raccogliere tutte le query rispetto ai documenti*** della collezione. La soluzione consiste nell'approssimare la probabilità di rilevanza come la probabilità di avere una determinata query dato un documento rilevante a priori:

$$
P(q | d,R=1)
$$

In questo modo si cerca di *conoscere la query che l'utente può fare se è interessato a un certo documento*.

##### Legge di Zipf

La ***distribuzione delle parole in un documento*** segue la **Legge di Zipf**. Secondo questa legge esistono pochi termini molto frequenti e molti termini poco frequenti. Dato il *rank di una parola* $r$ e la *frequenza della parola*, la legge si esprime come: 

$$
r * f = k
$$

##### Language Models

I Language Models si basano sulla Legge di Zipf per ***assegnare una probabilità ad una frase*** . Si utilizza per:

- Traduzione Automatica (capire il *senso della frase*)
- Correzione dello Spelling
- Riconoscimento Vocale

Oltre a calcolare la *probabilità di una frase o sequenze di parole*, sono in grado di ***calcolare la probabilità della parola successiva a certe parole o frase*** (es. *suggerimenti nella tastiera degli smartphone*). Il **problema** di quest’ultima operazione, che consiste nel rapporto tra il *conteggio delle volte che quella frase è comparsa prima della parola* e *il numero di volte che è presente la frase*, è che ***non è computazionalmente possibile*** data la grande quantità di dati. Per questo si utilizza lo **stemming**.

###### N-grammi

I **Language Models basati su *N-grammi*** approssimano la storia pregressa (frase precedente alla parola) con le ultime $N$ parole della frase (es. $2$ &rarr; *bigrammi*, $3$ &rarr; *trigrammi*). Si possono avere delle ***dipendenze a lunga distanza*** (es. *soggetto e verbo distanti nella frase*), ma questo modello fornisce comunque risultati accettabili. La **probabilità** si calcola utilizzando la **Maximum Likelihood Estimation** (**`MLE`**), ovvero contando *quante volte la frase compare rispetto alle parole precedenti nel documento*: 
$$
P(w_i|w_{i-1})=\frac{count(w_{i-1}w_i)}{count(w_{i-1})}
$$
La **probabilità di una frase** è data dalla *moltiplicazione delle singole probabilità dei bigrammi*:

> **Esempio: ** $P(\text{big data analysis}) = P(data|big) 	\times P(analysis|data)$

<u>**Problemi**</u>

Il modello è ***fortemente dipendente dalle frasi utilizzate** per l'allenamento*: più il *numero di parole* $N$ è grande, migliore sarà il risultato anche se sarà più **soggetto ad overfitting**. In alcuni ambiti può essere positivo (es. *suggerimenti della tastiera del telefono*) e in altri meno e sarà necessario creare un modello che generalizza maggiormente.

Un altro problema è quello degli **zeri**, che *azzerano la probabilità*. In questi casi si applica lo **smoothing**: si impedisce ad una frequenza di andare a zero aggiungendo una piccola quantità ad ogni valore. Per farlo è possibile utilizzare il **`Laplace Smoothing`**: aggiunge un $1$ al numeratore, mentre al denominatore viene aggiunto il numero di parole uniche trovate nel dataset (il *vocabolario* contenente le parole dei documenti). È abbastanza drastico e cambia i valori in modo molto notevole.
$$
P_{Laplace}(w_i|w_{i-1})=\frac{count(w_{i-1}w_i)+1}{count(w_{i-1})+V}
$$

###### Query Likelihood Retrieval Model

Supponendo che l'utente per una query utilizzi delle ***keyword** rappresentanti delle **parole estratte dal documento stesso***, questo presupposto permette di definire la probabilità condizionale di una query dato un documento senza l'utilizzo di una tabella di rilevanza. Supponendo inoltre le *parole nella query come indipendenti*, si ha che la **probabilità della query** è semplicemente il ***prodotto delle probabilità di ogni parola presente nella query***.

Quando un termine della query non compare nel documento porta la probabilità a zero. Una soluzione è utilizzare il language model con ***smoothing*** così se una parola non appare la inseriamo lo stesso con un valore molto basso. Per *applicare lo smoothing* si divide la **probabilità di una parola** nel documento in ***due termini***:

- se la ***parola è presente***: si fa il *conteggio all'interno del documento*
- se la ***parola non presente***: probabilità di avere la parola nella collezione di documenti moltiplicata per un *fattore* $\alpha$

$$
p(w|d)=\begin{cases}p_{Seen}(w|d) \quad \text{ if } w \text{ is seen in } d \\ \\ \alpha _ d p(w|C) \qquad \text{ otherwise}\end{cases}
$$

La funzione di ranking con lo smoothing può quindi essere riscritta come formata da:

1. la **somma** fra la *probabilità* che le *parole della query siano presenti* nel documento e la *probabilità che non siano presenti*
2. la **differenza** fra la *probabilità di tutte le parole nella collezione* e *le parole nel documento*

Per implementare questa funzione è necessario capire come impostare l’$\alpha$ e come stimare la probabilità (es. *per conteggio*) di una parola. Per far ciò si possono utilizzare alcune tecniche, tra cui:

- **`Jelinek-Mercer smoothing`**: in base ad un *parametro* $\lambda \in [0, 1]$ diciamo che se $\lambda = 1$ allora ha importanza la presenza della parola nella collezione, se $\lambda = 0$ ha importanza maggiore la frequenza dei termini dentro il documento.
- **`Smoothing Bayesiano`**: in base ad un parametro $\mu$ diciamo che se $\mu = 0$ si considera la *term frequency*, invece con $\mu >> 0$ prevale la *document frequency*.

###  Feedback

Un altro fattore da tenere in considerazione per migliorare il Text Retrieval è il feedback. È possibile utilizzare il feedback dell’utente per migliorare il sistema e la risposta. Le tipologie di Feedback di rilevanza sono:

- **`ESPLICITO`**: chiedere un parere direttamente all'utente
- **`PSEUDO RILEVANZA`**: si assume che i primi $k$ documenti siano rilevanti e si utilizzano questi $k$ per migliorare la query
- **`IMPLICITO`**: si estrae il giudizio dell’utente non esplicitamente, osservando i click dell’utente e le zone maggiormente viste o evidenziate nei documenti o nel riassunto del documento.

#### Rocchio Feedback

Strategia di feedback utilizzata nel `VSM`. L’utente fa una query $q$ che viene localizzata nello spazio: l’intorno dei documenti vicini a $q$ sono i documenti che vengono considerati con `VSM`. In questo intorno *si introduce il feedback* ***indicando con `'+'` i documenti rilevanti per la query, con `'-'` quelli non rilevanti***. Il **vettore** $q$ può essere ***spostato***, in modo tale che sia più vicino ai documenti rilevanti e più lontano da quelli non rilevanti così da ottenere un risultato migliore. Quindi la **nuova posizione** $q_m$ è data:
$$
\overrightarrow{q_m}=\alpha \cdot \overrightarrow{q}_{\text{ iniziale}}+d_{\text{ rilevanti}}-d_{\text{ non rilevanti}}
$$

## Implementazione Search Engine

I principali componenti del search engine sono di quattro tipi:

- **`TOKENIZER`**: determina la *rappresentazione del documento*. Il documento è *rappresentato come **vettore*** in cui ***ogni indice corrisponde a una parola***, per cui si *si registra **l'occorrenza***. Inoltre il *Tokenizer* assegna a ciascun documento un `ID`.

- **`INDEXER`**: consente di ***indicizzare i documenti con poca memoria***, ossia *dice quali documenti contengono una specifica parola*. Utilizzano come *componente principale* una **`inverted index`** che *data una parola restituisce i documenti che la contengono*. L'inverted index si compone di due parti:

  1. **`LEXICON`**: ***vocabolario*** come *insieme dei termini dei documenti*. Può essere messo in memoria, per velocizzare l'accesso.
  2. **`POSTING FILE`**: *tabella* che salva il collegamento tra il termine e i *documenti*

  Per creare un inverted index è possibile usare il MapReduce:

  1. `MAP` &rarr; vengono estratti i termini dai documenti (*chiave*) e per ciascuno si salva il *documento di provenienza* come `ID` (*valore*)
  2. `GROUP BY` &rarr; l’inverted file viene *ordinato alfabeticamente per termine e in ordine di documento* (`ID`).
  3. `REDUCE` &rarr; i termini ripetuti per uno stesso documento vengono uniti e si annota anche la frequenza di ogni termine. Da questo inverted file si può creare un file lexicon (salvato in memoria possibilmente), e il posting file (salvato su disco).

- **`SCORER/RANKER`**: *implementazione di un retrieval model per classificare i documenti*. Assegna *uno score ad ogni query termine per termine*, grazie all’indice inverso costruito precedentemente.
- **`FEEDBACK/LEARNER`**: *sistemi di feedback* per migliorare l'algoritmo tramite le informazioni implicite dell’utente.

## Text Classification

La Text Classification fa riferimento a quell'insieme di tecniche classificano il testo in modo da potergli **assegnare una *categoria*, un *argomento***. Può rilevare spam, identificare l’autore, il linguaggio, l'argomento del testo, o capire se si parla positivamente o negativamente di un argomento (Sentiment Analysis). Una definizione di alto livello definisce la Text Classification come un’operazione che ha come input un documento con un numero fissato di classi, e predice una classe appartenente alle classi in input. Esistono diversi metodi di classificazione:

- **`REGOLE IMPOSTATE MANUALMENTE`**: Si creano regole basate su combinazioni di parole. Può essere molto accurato se le regole sono ben impostate, ma sono difficili da gestire e mantenere.
- **`MACHINE LEARNING SUPERVISIONATO`**: dato un training set formato da $m$ documenti e un set di classi $Y=y_1,..,y_n$ il sistema si allena per predire a quale classe $y_i$ appartiene un documento $d$.

### Sentiment Analysis - Opinion Mining

La Sentiment Analysis è l'**analisi che studia le *opinioni***, le ***valutazioni*** e le ***emozioni*** delle persone attraverso testo, cercando principalmente di capire **se il sentimento è *positivo* o *negativo***. Esistono diversi livelli di analisi:

- **`DOCUMENTI`**: capire se un documento di grandi dimensioni (molte pagine) esprime un giudizio positivo o negativo riguardo un argomento. Si assume che un documento abbia un’*opinione binaria* e tratti una *singolo argomento*.
- **`FRASI`**: capire se una frase esprime un’opinione positiva, negativa o neutrale, quindi oggettiva o soggettiva.
- **`ENTITÀ`**: capire dal testo se un umano ha espresso un *giudizio* positivo o negativo per ogni singolo componente

Inoltre le opinioni possono essere:

- **`Opinioni Regolari`**: indicano un sentimento assoluto rispetto ad un’entità come singolo.
- **`Opinioni Comparative`**: indicano un’opinione su un’entità rispetto ad un’altra. Più difficili da analizzare

È possibile utilizzare un **vocabolario** (*Sentiment Lexicon*) di **termini o frasi *utilizzati frequentemente per esprimere pareri positivi o negativi***. Spesso questo non basta, perché bisogna tener conto anche di frasi sarcastiche, frasi senza opinioni, frasi che non esprimono un’opinione in quel contesto.

#### Definire in Modo Strutturato l’Opinione

Data una entità $E$ (*prodotto*, servizio, *argomento* o *evento*), essa è descritta come una coppia $E:(T,W)$:

- $T$ è una gerarchia di parti dell’entità (es. lente, batteria, obiettivo che compongono una fotocamera)
- $W$ è un insieme di attributi (*ciascuna parte o sotto parte ha il suo insieme di attributi*) 

Si può definire l’opinione come una quintupla (formula): 

1. $E$ è il nome dell’entità
2. $A$ è un aspetto dell'entità $E$
3. $S$ è il sentimento riguardante l’aspetto $A$
4. $H$ è il soggetto (*persona*) che emette l’opinione 
5. $T$ indica il *tempo* in cui l’opinione è stata espressa. Anche l'analisi in base al tempo è importante: un'opinione di due anni fa non è considerabile importante quanto una recente.

Una notazione di questo tipo *non è adatta alle opinioni comparative*. La definizione della quintupla fornisce un **metodo per trasformare un *testo non strutturato* in *dati strutturati***. La quintupla è uno **schema di database**, sulla base del quale le **opinioni** estratte possono essere collocate **in una *tabella***.

Vengono poi creati dei **report** delle opinioni in relazione agli aspetti, che sintetizzano i risultati delle quintuple. Per la classificazione del sentimento a livello di documento si usano o delle **tecniche di classificazione** o **tecniche non supervisionate**. 

- **`TECNICHE DI CLASSIFICAZIONE SUPERVISIONATE`**: quando il **sentimento** viene indicato come ***positivo o negativo***, allora si utilizzano ***classificatori di machine learning supervisionati***. È invece **regressione** quando il sentimento viene ***indicato con valori numerici*** (un *punteggio*) e viene predetto tramite un algoritmo di regressione. 
- **`TECNICHE DI CLASSIFICAZIONE NON SUPERVISIONATE`**: generalmente **basati su *clustering*** classificano come positivo un documento se la media dell’orientazione supera una soglia prefissata. Per determinare l’orientazione del sentimento, un metodo classico è quello di calcolare la differenza tra il **Pointwise Mutual Information** della frase con due sentimental words: si guarda la ***distanza della frase*** (in termini di *differenza*) tra un *termine positivo e uno negativo*.

## Computational Lexical Semantics

È lo studio della **semantica** del testo, ovvero al ***significato associato ad esso***. Viene utilizzata per diversi scopi:

- riconoscimento di ***termini simili***
- ***relazione tra vocaboli*** (es. *tazza* e *caffè*)
- **`World Sense Disambiguation`**, termini ambigui (es. *Jaguar*, auto o animale)
- riconoscimento del ***ruolo di un termine in una frase*** (*analisi logica di un termine*)
- ***connotazione delle parole***, utile per la *sentiment analysis*

Per lo studio vengono utilizzate alcune terminologie:

- **`Lemma`**: forma principale senza inflessioni di un termine
- **`WordForm`**: parola che presenta l’inflessione, così come appare nel testo
- **`Omonimia` e `Polisemia`**: la prima fa riferimento a *termini con più significati distanti tra loro*. La seconda a *termini diversi che possono avere lo stesso concetto* (es. *scuola*, *università*, *ospedale* come di *istituzione* o *edificio*)
- **`Metonimia`**: utilizzare un concetto di un termine per riferirsi ad altro (es. leggere un *autore*, ovvero le sue *opere*)
- **`Sinonimia`** o **`Antinomia`**: termini diversi con lo stesso significato o termini con il significato opposto
- **`Iponimia`** o **`Iperonimia`**: i termini possono essere *raggruppati per categoria* (es. automobile e bicicletta sono *veicoli*). I termini più specifici sono *iponimi*, mentre le categorie prendono il nome di *iperonimi*.

> **WordNet**
> Alcuni termini nei dizionari non possono essere spiegati in maniera semplice: indicazioni come *sinistra*, *destra*, oppure gli stessi *colori*, per i quali si utilizzano come esempio oggetti di quello stesso colore. Termini di questo tipo sono di ***difficile comprensione per un computer***. Occorre un **vocabolario** che per ogni termine *oltre alla definizione*, *salvi i termini ad esso più vicini*, in modo da **facilitare l'analisi del testo**. Tra questi vocabolari, uno dei più famosi è `WordNet`: un database che *per ogni termine*, oltre alla *definizione*, presente una *presenta una lista dei sinonimi e di altri termini rapportati ad esso*.

### Word Sense Disambiguation (`WSD`)

La `WSD` è la **tecnica** attraverso cui, *data una **parola in un contesto*** cerca di ***capire il suo significato tra quelli possibili***. Viene utilizzata nella *traduzione automatica* o per il *sintetizzatore vocale*. Possono essere prese in considerazione:

- **solo un sottoinsieme di parole**: questo metodo può essere utilizzato tramite delle tecniche di machine learning supervisionato
- **tutte le parole di un testo** (e quindi *tutti i loro significati*): è molto dispendioso per via della grande dimensione dei dati da gestire ed è improbabile che si possano avere dei dati sufficienti per ogni parola

#### Tecniche di ML Supervisionato

Le tecniche supervisionate **utilizzano un *classificatore*** come i **`Corpora`**: ***dataset pre-allenati*** che *forniscono delle parole pre-classificate a mano con il loro senso all'interno della frase*. Per l’estrazione delle features si può usare, *oltre alla `bag-of-worlds`* con solo le parole significative, la **`collocational features`** che guarda le *features delle parole vicine a quella target*.

#### Tecniche Semi Supervisionate Basate su Dizionari

Utilizza l'**algoritmo di Lesk semplificato**: dato un *dizionario che per un termine presenta più definizioni, vince la definizione che condivide più termini con la frase di riferimento*. Nella versione ***originale***, più complessa, si fa una *ricerca approfondita nel dizionario* utilizzando anche altri *termini* riportati nella *definizione del termine target*.

#### Metodo Basato sui Grafi

Il database di **WordNet** può essere visto come un ***grande grafo***. Per ogni **lemma** (*nodo*) si può creare una interconnessione con un altro lemma sulla base dei concetti di *iperonimia e iponomia*. Si associa il **risultato** che *minimizza la distanza fra due termini*.

### Similarità e Relazioni tra Parole

La **sinonimia** definisce in modo binario *se un termine è un sinonimo di un altro*, la **similarità** definisce una gradazione di quanto *due termini sono simili in un particolare contesto*. Il *calcolo della similarità* può essere utilizzato per ***selezionare documenti con parole simili a quelle utilizzate nella query***, e quindi potenzialmente poter fornire un numero maggiore di risultati: due parole simili spesso possono essere utilizzate come sinonimi. Esistono varie **tecniche per calcolare la *similarità***:

- **Basate sulla `DISTANZA` *(`Path Length`)***: tecnica che *valuta la distanza tra due termini*. La distanza viene calcolata costruendo l’albero di WordNet e risalendolo partendo dal primo termine target per arrivare sino al secondo termine target, ponendo $1$ al *numeratore*, mentre al *denominatore* si contano gli *archi che dividono i due termini* e ad essi si aggiunge $1$. È una tecnica molto semplice ma che si presta a tanti errori: si sta considerando il salto fra un path e l’altro sempre uguale a 1, però più si va verso l’astratto più questa distanza in genere aumenta. Inoltre si presuppone che la gerarchia sia corretta anche se non sempre vero. I limite sta nel fatto che non si hanno sempre tutti i **termini usati** nel linguaggio, i sinonimi, il fatto che questi si ***evolvono nel tempo*** e altri problemi legati all’uso dell’inflessione.

- **Basate sulla `DISTRIBUZIONE DELLE PAROLE`**: *stimando la somiglianza delle parole* trovando quelle con una *distribuzione simile in un documento* (es. `VSM)`. Per capire il ***significato della parola*** è possibile analizzare il **contesto** in cui viene utilizzata. 

  > **`EMBEDDING`**: termine che fa riferimento alla **rappresentazione di una parola in un contesto**. È un **vettore di numeri** (es. *colonne utilizzate nelle `bag-of-words`*).

#### Costruzione di Embedding

Per ciascun termine di interesse si costruisce la *bag of words* delle *altre parole utilizzate nel documento*. L'**individuazione di parole simili** avviene *confrontando gli embeddings*. L'embedding di una parola si può rappresentare tramite:

- **Rappresentazioni Sparse**: si utilizzano le *bag-of-words*. Si costruisce la ***matrice delle `co-occorrenze`*** $termini \times documenti$, in modo da individuare *documenti simili* (sulle colonne) e *termini simili* (sulle righe). Si utilizzano anche delle matrici $termini \times termini$: per ogni termine sulla riga si prendono un certo numero di parole successive e precedenti (*finestra* o *contesto*). Per ogni parola appartenente all'intorno di un termine o a un documento (a seconda della matrice) viene messo il valore $1$. Per **mantenere il contesto limitato** è bene *considerare poche parole sulle righe*, tenendo conto del *contenuto informativo delle parole* (evitando ad esempio le *congiunzioni*). La **frequenza delle parole** non è sempre una buona misura per le *relazioni tra parole*, per questo è possibile utilizzare la **Pointwise Mutual Information (`PMI`)**, che misura *quanto spesso un termine $w$ e un contesto $c$ occorrono insieme*, rispetto a *quanto occorrono in maniera indipendente*:
  $$
  PMI(w,c) = \log_2 \frac{P(w,c)}{P(w)P(c)}
  $$
  Il logaritmo proietta un *range di valori* da *negativi* (***poco probabile***) a $+\infin$ (***molto probabile***).

- **Rappresentazioni Dense**: *matrici più dense* (presentano meno zeri), con *meno colonne*. Si utilizzano le tecniche di **Singular Value Decomposition (`SVD`)**, meglio note come **Dimensionality Reduction**, che permettono di *approssimare una matrice sparsa in una versione più compatta*, creata eliminando le *features superflue* (rimangono i vettori densi). Tra questi modelli c’è:

  - `PCA`: data una *distribuzione di punti nello spazio*, si vuole capire quelli che sono gli *assi su cui si distribuisce l’informazione*. Si considerano solo alcune dimensioni, e si *riconducono le colonne alle dimensioni principali*.
  - `SVD`: esiste sempre una decomposizione di una matrice di input $A$ in tre matrici: $U$, $\Sigma$ e $V$. $U$,$V$ sono dense e hanno colonne ortonormali e $\Sigma$ è diagonale. Il prodotto di queste matrici permette di ricostruire approssimativamente la matrice originale. Il *Truncated* `SVD`, anche chiamato *Latent Semantic Analysis* (`LSA`), fa un’approssimazione: non considera $V$ e utilizza solo le prime $k$ dimensioni delle matrici, quindi limitando il $k$ si avrà una matrice ridotta. La matrice rappresentativa per funzionare deve mantenere almeno l'$85\%-90\%$ dei *valori precedenti*.

##### `WORD2VEC`

Algoritmo che ***crea $1$ vettore numerico per ogni parola***: ciascun vettore *combina diverse interpretazioni/significati della stessa parola*. Per addestrare i modelli è necessario disporre di un *corpus di documenti testuali quanto più ampio possibile*. Da questo corpus è possibile estrarre un vocabolario di parole distinte (***token***). Ogni token distinto è quindi rappresentato come un vettore $V$ con una *codifica One-hot encoding*, che ne rappresenta l'*interpretazione/significato*.

I **limite** principale è il fatto che la dimensione del vettore $V$ dipende dalla dimensione del vocabolario, che potrebbe essere anche molto ampia. Inoltre, se venissero aggiunti/rimossi token dal word embedding, il modello dovrebbe essere riaddestrato da zero. L'ultimo grosso limite di questo approccio è il fatto che non viene risolto il problema della polisemia dei termini.

Word2vec può produrre word embedding utilizzando uno dei due modelli di architettura: **Continuous Bag-of-Words** (**`CBOW`**) e **Skip-Gram**. I due modelli esplorano il *contesto del token investigando gli altri token che sono prossimi a quest'ultimo*. Qui si evidenzia la principale differenza tra i due modelli architetturali:

- **`CBOW`** mira a predire il *token corrente* (output) a partire *da una finestra di parole di contesto* (input).
- **Skip-Gram** ha lo scopo di predire le *parole di contesto* (output) a partire dal *token corrente* (input).

Il parametro $C$ definisce la dimensione della finestra di contesto: sono incluse nel contesto i $C$ token immediatamente precedenti/successivi al token corrente. In entrambi i modelli *non viene tenuto conto della posizione nel testo dei token di contesto*.

##### `BERT`

Bidirectional Encoder Representations from Transformers (BERT) è lo stato dell’arte attuale ed è un modello di machine learning basato sui transformer ed utilizzato nell'elaborazione del linguaggio naturale (NLP). `BERT` genera ***embeddings contestualizzati***, ovvero **un vettore** ***per ciascun significato di una parola***, quindi ogni parola può avere più embeddings.

`BERT` si basa su un trasformer che è composto da:

- `ENCODER`: data una frase in input, genera una rappresentazione intermedia e la da in pasto all’encoder del layer successivo
- `DECODER`: decodifica la rappresentazione intermedia e ne fornisce un output

`BERT` è stato allenato su due task:

- **`MASKED TOKEN PREDICTION`**: utilizza un token speciale per *mascherare una parola all’interno della frase*, e la *rete viene allenata per predire la parola nascosta*
- **`NEXT SENTENCE PREDICTION`**: data in input una *coppia di frasi* che hanno i vari token (*parole*), in output ritorna una predizione che *dice se la seconda frase è la successiva della prima o no*
