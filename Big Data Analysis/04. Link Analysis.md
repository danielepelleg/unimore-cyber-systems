# 4. Link Analysis

## PageRank

Il PageRank è un **algoritmo** introdotto da *Google* utilizzato ***per classificare i risultati di un motore di ricerca***. L’obiettivo di un motore di ricerca è quello di dare come primi risultati quelli più interessanti per l’utente.

I **primi motori di ricerca** effettuavano una scansione del Web inserendo le *parole chiave dei documenti* in un'inverted index, creando un *puntatore tra parole &rarr; pagine*. La presenza di uno o più termini nell’header della pagina la rende più rilevante: qualcuno potrebbe approfittarsi di questo fatto per attirare degli utenti in modo sleale. Per risolvere questo problema è possibile guardare quante pagine presentano come riferimento quella stessa pagina (*link analysis*).

La **link analysis** è il ***principio*** su cui si basa il *PageRank*: se una **pagina** che parla di un argomento è ***linkata da tante altre pagine*** che parlano di quell’argomento, allora la pagina è un’autorità nel campo, e quindi è ***interessante***. Per valutare l’importanza di un pagina, bisogna vedere se chi linka a sua volta è linkato da tante altre pagine.

È possibile vedere i **link** come dei ***voti***, dove il voto è proporzionale all’importanza di una pagina. Il **voto** di una pagina $j$ con importanza $r(j)$ e $n$ link in uscita vale ***$\frac{r(j)}{n}$***, mentre la sua **importanza** è uguale alla ***somma dei voti dei link in entrata***. Il *voto di una pagina importante* viene *pesato maggiormente*: una pagina è importante se è linkata da altre pagine importanti.

### Calcolo del PageRank

#### Processo di Markov

Con questo procedimento il **web** viene *rappresentato* mediante una ***matrice di transizione* $M$** (anche chiamata *stocastica* o *matrice di Markov*), ovvero una matrice che la cui somma degli elementi su ogni colonna (*stati di arrivo*) è uguale ad $1$. La matrice rappresenta un grafo avente *come nodi $[A,B,C,D]$ le pagine web*: le ***colonne*** rappresentano i *link in uscita da una pagina*, mentre le ***righe*** i *link in entrata*. Ogni **elemento** $(i,j)$ della matrice  rappresenta la ***probabilità di passare dalla pagina $j$ alla pagina $i$***. Essendo delle probabilità, ciascun elemento della matrice è compreso tra $[0,1]$.

Considerato un **vettore $r$** (*rank vector*) che identifica l’insieme dei nodi $[A,B,C,D]$: ciascun valore del vettore rappresenta la ***probabilità di partire da un determinato nodo***. Dal prodotto $M \times r$ si ottiene il **vettore $r’$** che rappresenta la *probabilità*, partendo *da un qualsiasi nodo della rete rappresentata mediante una matrice $M$*, di *arrivare negli altri nodi della rete*. Il prodotto $M \times r$ rappresenta il sistema contenente le **equazioni di flusso** della rete.

Se si **itera l'operazione $r' = M \times r$** mediante un **power method** che *simula una navigazione casuale all'interno della rete*, e si sostituisce di passo in passo $r$ con $r'$, dopo un certo numero di iterazioni si arriva ad una situazione in cui i valori del vettore $r$ non variano più: $r^t \simeq r^{t-1}$, pertanto ***si raggiunge una situazione di stazionarietà***. Una volta giunti a questo punto il *vettore $r$ rappresenta* il PageRank (importanza) finale dei nodi della rete.

Si tratta di un **processo Markoviano**: la *probabilità di essere in uno stato al tempo $t+1$ dipende dal tempo $t$*. Affinché l'applicazione delle moltiplicazioni $r' = M \times r$ converga ad un unico vettore stazionario, la matrice deve essere:

- **`STOCASTICA`** : per *definizione*, avendo come somma delle colonne $1$
- **`APERIODICA`** : *non ci sono cicli all'interno del grafo* (es. $A\rightarrow B\rightarrow C\rightarrow D\rightarrow A$)
- **`IRRIDUCIBILE`** : *assenza di **vicoli ciechi**, pagine non connesse ad altre pagine (no link uscenti, solo entranti)*

##### Problemi

Il web non è sempre *stocastico, irriducibile e aperiodico*. Esistono delle tecniche per eliminare questi problemi:

1. **Eliminare i Vicoli Ciechi**

   Questo metodo **potrebbe portare a un'eliminazione di nodi in cascata** siccome ***vengono tagliati degli archi***. È una ***soluzione non scalabile*** per questo motivo **non viene utilizzata**, anche se porta ad una situazione senza vicoli ciechi. Una volta calcolato il PageRank per i nodi rimanenti, si riaggiungono i nodi e gli archi precedentemente tolti e si calcola il PageRank di questi ultimi nodi sfruttando le connessioni che hanno con gli altri nodi precedentemente calcolati.

2. **Utilizzare la `taxation`**

   Si tratta di una *trasformazione della formula* che *introduce un elemento addizionale*:
   
   $$
   r' = \beta Mr + (1 - \beta) \frac{e}{n}
   $$
   
   Dove $\beta$ è una costante compresa tra $[0.8 , 0.9]$, $e$ è un *vettore* di $1$, e $n$ è il *numero di nodi*. L'elemento $\frac{e}{n}$ del secondo addendo è un *vettore della stessa dimensione di $r$* in cui ogni termine è pari a $\frac{1}{n}$.

   Il **primo addendo** $\beta M r$ rappresenta la ***probabilità che un navigatore casuale possa seguire i link della rete***, leggermente inferiore per via di $\beta$. Se prima questa probabilità si verificava con probabilità certa, ora si verifica nel $80-90\%$ dei casi.

   Il **secondo addendo** $(1-\beta)\frac{e}{n}$ indica una ***probabilità*** molto piccola, intorno al $10-20\%$ che ***l’utente non segua i link della rete*** ma ***navighi casualmente in un qualsiasi altro degli $n$ nodi*** della rete: prende il nome di **`Teleport`**. Con il Teleport si *risolve il problema*, siccome la nuova formulazione **rende la matrice *stocastica*, *irriducibile* e *aperiodica***.

   - _Stocastica_ : ogni colonna avrà sicuramente somma 1.

   - _Aperiodica_ : ogni nodo avrà un Teleport (***arco verso se stesso***) che può ***rompere la periodicità del grafo***

   - _Irriducibile_ : tutti i nodi avranno una minima probabilità di raggiungere qualunque altro nodo, non si avrà mai probabilità $0$.


### Topic-Sensitive PageRank

Il *PageRank* può essere utilizzato in un ***search engine*** (*motore di ricerca*) per *decidere l'ordine di visualizzazione delle pagine*: così facendo le pagine con un *valore di page rank maggiore* vengono messe in *cima alla lista*. È possibile costruire dei **motori di ricerca più performanti** utilizzando il ***teleport***, facendo in modo che l'utente possa navigare in maniera casuale solo su un numero limitato di pagine: il *vettore $e$* in questo modo assume il valore $1$ solo in corrispondenza di certe pagine.

Nei motori di ricerca più performanti vengono utilizzati **Topic Sensitive PageRank**, cioè dei *PageRank relativi a un certo topic*. Modificando la formula del PageRank si può *forzare la navigazione di un utente* su *pagine relative a un certo topic considerate di suo interesse*. Supponendo che certi nodi parlino di un determinato argomento: modificando il vettore $e \rightarrow e_S$ inserendo al suo interno $1$ solo per le pagine che parlano di un certo argomento e si divide per il numero di pagine $S$ (numero di pagine totali di quell’argomento), si ottiene $\frac{e_S}{S}$. Si crea una rete in cui i salti casuali avvengono solo nelle pagine della rete che riguardano il topic. Le pagine inerenti all'argomento della ricerca avranno infatti un PageRank maggiore, e la formula sarà:

$$
r' = \beta Mr + (1 - \beta) \frac{e_S}{S}
$$

### Utilizzo del MapReduce

Il *Map Reduce* viene utilizzato per *calcolare il PageRank* nel caso in cui il **Web** sia *rappresentato* da ***grafi di grandi dimensioni***: in questi casi il *prodotto matrice per vettore viene fatto con il Map Reduce*.

Nel caso del caso del **Web** la **matrice $M$ è molto sparsa**: *non tutti i nodi hanno molti collegamenti*, ed effettuare $50$ iterazioni (in media) per trovare il PageRank, diventa molto dispendioso dal punto di vista computazionale. Un miglioramento lo si ha utilizzando una rappresentazione diversa della matrice $M$ in cui ***si mantengono solo gli elementi diversi da $0$***: solo gli elementi $\neq 0$ contribuiscono al calcolo del PageRank. È possibile rappresentare la matrice in forma tabellare, mediante tre diverse colonne:

- **`SORGENTE`** : *nodo sorgente considerato*
- **`DESTINAZIONE`** : *nodi di arrivo*
- **`DEGREE`** : *numero di nodi in cui il nodo sorgente si suddivide*, il *suo numero di destinazioni* 

Una rappresentazione di questo tipo *ottimizza l'uso della matrice stessa*, e quindi il *calcolo*.

Il ***MapReduce*** può essere utilizzato per fare il calcolo *matrice per vettore* $M \times r$, ma questo ***non funziona se $r$ è troppo elevato***: anche suddividendo in colonne la matrice e in righe il vettore, il risultato memorizzato in $r'_1$ e $r_2'$ non starebbe in memoria, dato che $r$ non sta in memoria. Una **soluzione** consiste nel ***suddividere la matrice, oltre che in colonne, anche in righe***. La matrice viene così divisa in $k^2$ blocchi e il vettore in $k$ righe, di modo che il prodotto tra $r_i$ e $M_{ij}$ sia inserito in $r_i'$. Scegliendo il valore $k$, ad ogni mapper viene assegnato un *riquadro della matrice*. Utilizzando un Combiner è possibile fare la somma dei vari blocchi così da dare meno lavoro computazionale al Reducer e trasportare meno dati.

### Link Spam

Il **Link Spam** è una tecnica utilizzata per ***aumentare il PageRank di una pagina***.

Si ha una ***pagina target*** $t$, che l'utente vuole far diventare rilevante, e un gran numero $m$ di ***supporting pages*** sempre appartenenti allo stesso utente. Ciascuna delle *$m$ supporting pages* presenta un *link alla pagina target*, che a sua volta, per evitare vicoli ciechi, presenta *un link per ciascuna delle $m$ pagine*. Per innalzare di molto il PageRank, è necessario che anche una **pagina importante** (**accessible page**) presenti un *link alla pagina target* (es. spam di un link nei commenti di un blog). Ciò è dovuto al fatto che il voto di una pagina importante viene pesato maggiormente. 

Il PageRank $y$ del target $t$ viene quindi alimentato da tre sorgenti:

- Dalle pagine che vengono dall’esterno (indicate con $x$)
- Da $\beta$ volte il PageRank di ognuna delle $m$ pagine di supporto: $\beta (\frac{\beta y}{m} + \frac{1 - \beta}{n})$
- Dall’elemento di teleport $\frac{(1-\beta)}{n}$ (non considerato ora)

$$
y = x + \beta m (\frac{\beta y}{m} + \frac{1 - \beta}{n}) \rightarrow y = \frac{x}{1 - \beta^2} + \frac{\beta}{1 + \beta^2}\frac{m}{n}
$$

Se consideriamo $\beta = 0,85$ il PageRank aumenta di circa $3.6$ volte. Quindi utilizzando un **$m$ molto grande** è possibile *aumentare il PageRank* $y$ della pagina *target* $t$, e quindi ***imbrogliare l’algoritmo del PageRank***.

#### Contromisure

- **Controllare la Cronologia della Rete**
- **TrustRank**: si crea un *topic sensitive page rank* di *pagine fidate*. In questa rete il Teleport riporta solo questi siti affidabili.
- **Spam Mass**: identifica pagine spam, eliminandole o abbassandone il PageRank. Nelle *TrustRank* per individuare una pagina spam, è possibile calcolare lo **spam mass** che è uguale a $\frac{r - t}{r}$ dove $r$ è il ***PageRank*** della pagina e $t$ è il ***TrustRank***. Un valore negativo o piccolo indica che la pagina non è spam. Se lo spam mass è vicino ad $1$, molto probabilmente la è.

## Algoritmo di Hubs e Authorities - `HITS`

Il PageRank non è l’unico indicatore che si può utilizzare: è possibile *classificare le pagine con il concetto di  **hub** e **authority***. L'**importanza** di una pagina si misura mediante due parametri:

- **`HUB`** : aumenta *se la pagina presenta dei link a delle pagine che sono delle authority* in un certo argomento. Un **`hub`** è una *pagina che punta a più pagine*. È la ***somma dei voti delle authority a cui punta***.
- **`AUTHORITY`**: aumenta se *la pagina è linkata da molte pagine*. Un’authority è una *pagina puntata da molte altre pagine*. È la ***somma dei voti degli hub da cui è puntata***.

L’algoritmo utilizzato si chiama **`HITS`** (Hyperlink-Induced Topic Search).

1. Si assegna un $1$ ad ogni pagina di hub. 
2. L'authority si calcola sommando gli hub dai quali è linkata $(1 + 1 + ... + 1)$. Se una pagina (es. *Amazon*) è linkata da tre hub, in questo modo acquisisce importanza $3$.
3. L'authority fa assumere agli hub che lo citano il valore precedentemente calcolato. A questo punto gli *hub* assumono come valore la *somma dell'importanza delle authority a cui punta*. 
4. Si ricalcola l'authority come somma dei nuovi valori assunti dagli hub.

Il principale **problema**, nonché motivo per cui *questo classificatore non viene utilizzato*, è che **nella realtà la *rete è bipartita***: nell'esempio analizzato non si arriverà mai ad un bilanciamento, nel caso reale dopo una serie di passaggi si bilancia tutto e la rete dopo un po’ di tempo converge. Inoltre è difficile sapere se un ***utente sia interessato a un hub o un authority*** quando *effettua una ricerca*.

Un buon hub linka molte autorità, mentre una buona authority è linkata da molti hub. Gli **score di hub e authority** vengono **rappresentati come *vettori*** $h$ e $a$ per ogni nodo. Per il calcolo si utilizza la matrice $L$ che rappresenta il Web: questa matrice non è altro che la ***trasposta della matrice $M$ utilizzata dal PageRank***: le **colonne** rappresentano ora i ***link in entrata in una pagina***, mentre le **righe** i **link in uscita**. Ciascun elemento della matrice $l_{ij}$ può assumere i valori $1$ o $0$, a seconda se è presente un link tra la pagina $i$ e la pagina $j$. Per calcolare **hubs** e **authorities** si iterano questi due step:

1. `AUTHORITY` &rarr; si calcola $a = L^T \times h$ e poi si normalizza in modo tale che la componente più grande sia $1$.
2. `HUB` &rarr; si calcola $h = L \times a$ e si scala di nuovo.
