- [Processo](#processo)
    - [Esempio di Chiamata di Sistema](#esempio-di-chiamata-di-sistema)
  - [`EXEC` Pointer](#exec-pointer)
  - [Sistemi a Tempo Condiviso](#sistemi-a-tempo-condiviso)
  - [Duplice natura del Processo](#duplice-natura-del-processo)
    - [Flusso di Esecuzione](#flusso-di-esecuzione)
    - [Proprietà delle Risorse (Ownership)](#proprietà-delle-risorse-ownership)
  - [Thread vs. Processo](#thread-vs-processo)
    - [Modello Single Thread](#modello-single-thread)
    - [Modello Multithread](#modello-multithread)
      - [Esempio di un Applicazione Word](#esempio-di-un-applicazione-word)
- [Concorrenza](#concorrenza)
  - [Shared Memory](#shared-memory)
    - [Mutua Esclusione](#mutua-esclusione)
      - [Atomicità](#atomicità)
        - [Esempio: Variabile x](#esempio-variabile-x)

# Processo
Un processo è un programma in esecuzione. Sono programmi caricati in memoria principali, attivi, ma non necessariamente utilizzano la CPU. Quando un architettura esegue un processo, questo attraversa diversi *stati*. Per *stato di un processo* si intende lo stato (il valore) delle sue variabili. Ogni volta che un programma viene eseguito, viene creato un *nuovo processo*. Il sistema operativo classifica il processo attraverso diverse etichette:
- starting
- ready
- executing
- blocked
- terminating

Se si ha un solo core, ci sarà un solo processo in esecuzione. Il termine *blocked* a che fare con il concetto di sincronizzazione: in questo stato il processo è in attesa di qualcosa. Lo scheduler è quel modulo del sistema operativo che osserva tutti i processi del sistema operativo scegliendone uno, o più di uno in caso di sistemi multicore, da mandare in esecuzione (*dispatching*). E' importante salvare lo stato e le variabili di un processo prima di una *preemption* (messa in esecuzione di un altro processo), in questo modo l'esecuzione può ripartire da dove si era fermata. Le risorse possono essere condivise tra i vari processi. Talvolta è opportuno coordinare i problemi di mutua esclusione mediante degli strumenti del sistema operativo che prendono il nome di semafori. Andare da running a blocked significa sospendere il processo.

Lo scheduler guarda quali processi sono pronti nel sistema operativo, mantiene questa coda ordinata o ha una policy per scegliere chi mandare in esecuzione e a quello da eseguire assegna la CPU. Talvolta qualcuno si blocca e finisce pertanto nella coda dei task bloccati.

### Esempio di Chiamata di Sistema
Il cambio di contesto avviene quando si passa da user mode a supervisor mode, ma anche quando si passa da un processo a un altro. `open`, `read`, sono tutte chiamate che invocano il sistema operativo, anche questi sono esempi di *cambio di contesto*. E' il sistema operativo che accede a queste determinate risorse condivise al posto nostro nella maniera più opportuna, facendo il controllo degli accessi e utilizzando API apposite. Noi utenti non possiamo fare assegnamenti di memoria fuori dal nostro segmento di memoria (*segmentation fault*).

```c
    int fd,n;
    char buff[100];

    fd = open("Dummy.txt", O_RDONLY);
    n = read ...
```
Quando arriva un interrupt viene salvato lo stato dei registri, viene gestito l'interrupt e si riparte. Questo procedimento viene ripreso anche quando si cambia un processo. Uno *scheduler preemptive* può interrompere un processo prima che questo abbia terminato la sua esecuzione. Il vantaggio di uno scheduler preemptive può essere quello della priorità, oppure il blocco di un processo: se si hanno dei problemi con un processo questo viene bloccato e ne viene eseguito un altro. Un altro vantaggio è la *fearness*, si sospende un processo lento per favorirne l'esecuzione di uno più veloce. Uno scheduler *non preemptive*, al contrario, evita il *context switch*. Ogni preemption ha 2+2 cambi di contesto, salvo e metto quello dell'altro e al ritorno salvo e metto quello del processo precedentemente sospeso.

## `EXEC` Pointer
C'è un puntatore nel Sistema Operativo che punta al processo in stato di esecuzione. Quando avviene un *process switch*, il processo in fase di running diventa *blocked* o *ready*. Se è stato interrotto da un semaforo viene messo in stato di blocked, mentre se ha esaurito il suo quanto di tempo (scheduler round robin) o in seguito all'arrivo di un processo con priorità maggiore, va in stato di *ready*. Punta al PID del processo in esecuzione.
1. Salvataggo dei parametri nello stack (i registri). Questi parametri sono quelli che verranno salvati nel PCB.
2. Cambio in supervisor mode
3. Salvataggio del contesto nel PCB
4. Cambio dello stato del processo (*ready* o *blocked*)
5. Chiamata allo scheduler
6. ...

## Sistemi a Tempo Condiviso
Ciascun processo esegue per un lasso di tempo determinato (ad esempio 10msec). Lo scheduler in questo caso viene detto *round robin* o *time sharing*: ciascun processo ha a disposizione un round. Alla fine del round, il processore viene assegnato ad un altro processo. Se il processo non ha finito viene accodato. Il *timer interrupt* è un cambio di contesto, non solo tra processi diversi ma anche con il sistema operativo. Quando arriva l'interrupt:
1. Si passa a supervisor mode
2. Si salva il CR e l'IP
3. Si salva il contesto
4. Si esegue l'interrupthandler
5. Si chiama lo scheduler

Un cambio di contesto può essere volontario (il processo stesso decide di bloccarsi), come nel caso di una *read*. Una volta che esegue il sistema operativo non è detto che tocchi di nuovo al processo appena sospeso a eseguire: normalmente si invoca lo scheduler. Quando il cambio di contesto non è volontario è normalmente dovuto a un interrupt, come lo scadere di un quanto di tempo o l'arrivo di un processo I/O che sblocca un processo con priorità maggiore.

## Duplice natura del Processo
### Flusso di Esecuzione
Ogni processo nei sistemi multithreading / multiprocessing ha il proprio flusso di esecuzione. Le priorità determinano questi flussi.

### Proprietà delle Risorse (Ownership)
Un processo è proprietario di determinate risorse, come ad esempio dello spazio di indirizzamento.
- Spazio di indirizzamento virtuale
- Immagine del processo (codice + dati)
  
Se più processi vogliono scrivere o leggere contemporaneamente è il sistema operativo che si occupa di arbitrarli.

## Thread vs. Processo

Con il termine `THREAD` si fa riferimento al flusso di esecuzione (si immagini un filo), mentre con il termine `PROCESSO` si fa riferimento al *proprietario delle risorse*. Normalmente si hanno *più flussi di esecuzione* tutti nello stato di memoria appartenente a un processo. I thread prendono anche il nome di *lightweight processes*. Mentre un processo può avere più thread, un thread può appartenere a *un solo processo*.

### Modello Single Thread
A un processo corrisponde un solo thread.
- uno spazio di indirizzi
- uno stack
- un PCB

Ogni processo ha un'*user stack* e un *kernel stack* dedicato per le chiamate di sistema.

### Modello Multithread
Ogni processo può avere più thread:
- uno spazio di indirizzamento
- un PCB ma multipli TCB (thread control block)
- più stacks
- i thread sono gestiti da uno scheduler globale

Ogni thread ha il suo stack composto dalla parte user e dalla parte kernel. La parte di memoria stack è privata per ciascun thread. La memoria globale (*heap*), può essere invece condivisa da più flussi di esecuzione. I flussi in questo modo possono leggersi e scriversi i dati condivisi. Processi differenti possono condividersi variabili globali solo attraverso *chiamate di sistema*. Si ha un incremento delle prestazioni con i thread: più veloci, d'altro canto dal punto di vista della sicurezza è peggio. Un thread che da problemi potrebbe portare i suoi problemi anche agli altri. Anche i *thread context switch* sono nettamente più veloci rispetto a un cambiamento di processo. I processi sono per lo più utilizzati per *competere* per l'ottenimento delle risorse, mentre dei thread vengono utilizzati per *collaborare* al fine di raggiungere un obiettivo.

#### Esempio di un Applicazione Word
- Attesa dell'Input dell'utente
- Aggiornamento del Documento
- Formattazione del Documento
- Controllo errori di sintassi
- Controllo altri eventi (salvataggio automatico)

In questo caso l'utilizzo di un singolo processo sarebbe uno spreco di tempo. Una soluzione potrebbe essere quella di sfruttare i tempi di attesa (che rappresenta la maggior parte del tempo). Mentre si aspetta un task sarebbe bene portarsi avanti con gli altri. Se lo dividessimo su più processi ci sarebbe la problematica di interfacciarsi su un unico documento quando hanno tutti spazi di indirizzamento separati. Solitamente si costruisce un processo owner che funziona da server: è proprietario del documento e gli altri per interfacciarsi ad esso lo interrogano. Questo approccio protegge in caso di funzionamenti ma è una soluzione *lenta*: troppe chiamate di sistema. La soluzione ideale consiste nell'utilizzo di un unico processo e più threads. Unic processo proprietario della risorsa documento e diversi core per i task di input, format, sintassi, grafica. La concorrenza aiuta non solo sulla divisione su più core ma anche sul singolo core. Quando uno di questi thread è bloccato gli altri possono proseguire.

Lo scheduler gestisce i flussi di esecuzione (i thread): è importante ricordare che il processo è il caso degenere con un solo thread.

# Concorrenza
Con il termine si fa riferimento alla collaborazione di thread che concorrono per lo stesso obiettivo. Il processore non è mai in stato IDLE e viene sempre assegnato a qualcuno in attesa. la concorrenza può essere:
- **COMPETIZIONE**: un server è proprietario di risorse e ne regola l'accesso mediante determinate API. Le attività di competizione devono essere *protette*. Se un processo deve accedere alla risorsa, manda una richiesta al server. Le azioni sono regolate dalle due principali primitive `send` e `receive`. La competizione viene implementata normalmente mediante *message passing*. Il message passing è molto semplice: si chiama una send o una receive.
- **COOPERAZIONE**: l'allocazione della risorsa non è più centralizzata. Ogni thread può accedere alla risorsa: qui i thread possono cambiarsi il terreno sotto ai piedi a vicenda (corse critiche, accesso sincronizzato, mutua esclusione). la responsabilità della correttezza del programma non è demandata a un server centrale, ma a ciascun thread. Un thread che accede a una sezione critica *deve sempre proteggerla*. La cooperazione viene implementata normalmente mediante *shared memory*. La shared memory è difficile da implementare, ma porta a un incremento non indifferente delle prestazioni.

## Shared Memory
In generale è possibile allocare una risorsa *staticamente* o *dinamicamente*. Se statica una volta assegnata una risorsa a un processo non può essere revocata. L'accesso può essere dedicato (a un singolo thread) o condiviso (a più thread).

### Mutua Esclusione
Ogni thread con le sue tempistiche legge e scrive su buffer condivisi. Non sappiamo quando va in esecuzione ciascun thread: è un sistema asincrono.

#### Atomicità
Consente di essere sicuri che nel momento in cui si esegue un'operazione (in scrittura o in lettura) su una risorsa, nessun altro thread può eseguire una qualsiasi altra operazione su di essa. Un'operazione atomica non può essere interrotta.

##### Esempio: Variabile x
Shared Memory
```c
x = x + 1
```
Stack 1
```c
void *thread(void *){
    ...;
    x = x + 1;
    ...;
}
```
Stack 2
```c
void *thread(void *){
    ...;
    x = x + 1;
    ...;
}
```
Cattivo Interlacciamento - Assembly
```assembly
LD R0,x TA  x = 0
LD R0,x TB  x = 0
INC R0  TB  x = 0
ST,x R0 TB  x = 1
INC R0  TA  x = 1
ST,x R0 TA  x = 1
```