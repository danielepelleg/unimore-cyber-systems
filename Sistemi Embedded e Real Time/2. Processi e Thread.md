- [Processo](#processo)
    - [Esempio di Chiamata di Sistema](#esempio-di-chiamata-di-sistema)
  - [`EXEC` Pointer](#exec-pointer)
  - [Sistemi a Tempo Condiviso](#sistemi-a-tempo-condiviso)
  - [Duplice natura del Processo](#duplice-natura-del-processo)
    - [Flusso di Esecuzione](#flusso-di-esecuzione)
    - [Proprietà delle Risorse (Ownership)](#proprietà-delle-risorse-ownership)
  - [Thread vs. Processo](#thread-vs-processo)
    - [Modello Single Thread](#modello-single-thread)
    - [Modello Multithread](#modello-multithread)
      - [Esempio di un Applicazione Word](#esempio-di-un-applicazione-word)
- [Concorrenza](#concorrenza)
  - [Shared Memory](#shared-memory)
    - [Mutua Esclusione](#mutua-esclusione)
      - [Atomicità](#atomicità)
        - [Esempio: Variabile x](#esempio-variabile-x)
- [Consistenza](#consistenza)
    - [Esempio: Array Circolare](#esempio-array-circolare)
      - [Quando è consistente?](#quando-è-consistente)
- [Correttezza](#correttezza)
- [Sezioni Critiche](#sezioni-critiche)
- [Problema Produttore-Consumatore](#problema-produttore-consumatore)
- [Semaforo](#semaforo)
  - [Wait and Signal](#wait-and-signal)
  - [Sincronizzazione](#sincronizzazione)
  - [Implementazione](#implementazione)
    - [RISC V (Five) "atomic" support](#risc-v-five-atomic-support)
      - [Esempio](#esempio)
- [Monitors](#monitors)
  - [Condition Variables](#condition-variables)
- [Message Passing](#message-passing)
  - [Risorse e Scambio di Messaggi](#risorse-e-scambio-di-messaggi)
    - [Esempio: Comunicazione Sincrona](#esempio-comunicazione-sincrona)
    - [Esempio: Comunicazione Asincrona](#esempio-comunicazione-asincrona)
  - [Receive Simmetrica e Asimettrica](#receive-simmetrica-e-asimettrica)
  - [Remote Procedure Call (RPC)](#remote-procedure-call-rpc)
  - [Send e Receive Sincrone](#send-e-receive-sincrone)
  - [Send Asincrona e Receive Sincrona](#send-asincrona-e-receive-sincrona)
- [Deadlocks](#deadlocks)
  - [Risorse Consumabili e Riutilizzabili](#risorse-consumabili-e-riutilizzabili)
  - [Condizioni per il Deadlock](#condizioni-per-il-deadlock)
  - [Soluzione al Problema del Deadlock](#soluzione-al-problema-del-deadlock)
    - [Deadlock Prevention](#deadlock-prevention)
      - [Condizioni](#condizioni)
  - [Stretegie di Recupero](#stretegie-di-recupero)

# Processo
Un processo è un programma in esecuzione. Sono programmi caricati in memoria principali, attivi, ma non necessariamente utilizzano la CPU. Quando un architettura esegue un processo, questo attraversa diversi *stati*. Per *stato di un processo* si intende lo stato (il valore) delle sue variabili. Ogni volta che un programma viene eseguito, viene creato un *nuovo processo*. Il sistema operativo classifica il processo attraverso diverse etichette:
- starting
- ready
- executing
- blocked
- terminating

Se si ha un solo core, ci sarà un solo processo in esecuzione. Il termine *blocked* a che fare con il concetto di sincronizzazione: in questo stato il processo è in attesa di qualcosa. Lo scheduler è quel modulo del sistema operativo che osserva tutti i processi del sistema operativo scegliendone uno, o più di uno in caso di sistemi multicore, da mandare in esecuzione (*dispatching*). E' importante salvare lo stato e le variabili di un processo prima di una *preemption* (messa in esecuzione di un altro processo), in questo modo l'esecuzione può ripartire da dove si era fermata. Le risorse possono essere condivise tra i vari processi. Talvolta è opportuno coordinare i problemi di mutua esclusione mediante degli strumenti del sistema operativo che prendono il nome di semafori. Andare da running a blocked significa sospendere il processo.

Lo scheduler guarda quali processi sono pronti nel sistema operativo, mantiene questa coda ordinata o ha una policy per scegliere chi mandare in esecuzione e a quello da eseguire assegna la CPU. Talvolta qualcuno si blocca e finisce pertanto nella coda dei task bloccati.

### Esempio di Chiamata di Sistema
Il cambio di contesto avviene quando si passa da user mode a supervisor mode, ma anche quando si passa da un processo a un altro. `open`, `read`, sono tutte chiamate che invocano il sistema operativo, anche questi sono esempi di *cambio di contesto*. E' il sistema operativo che accede a queste determinate risorse condivise al posto nostro nella maniera più opportuna, facendo il controllo degli accessi e utilizzando API apposite. Noi utenti non possiamo fare assegnamenti di memoria fuori dal nostro segmento di memoria (*segmentation fault*).

```c
    int fd,n;
    char buff[100];

    fd = open("Dummy.txt", O_RDONLY);
    n = read ...
```
Quando arriva un interrupt viene salvato lo stato dei registri, viene gestito l'interrupt e si riparte. Questo procedimento viene ripreso anche quando si cambia un processo. Uno *scheduler preemptive* può interrompere un processo prima che questo abbia terminato la sua esecuzione. Il vantaggio di uno scheduler preemptive può essere quello della priorità, oppure il blocco di un processo: se si hanno dei problemi con un processo questo viene bloccato e ne viene eseguito un altro. Un altro vantaggio è la *fearness*, si sospende un processo lento per favorirne l'esecuzione di uno più veloce. Uno scheduler *non preemptive*, al contrario, evita il *context switch*. Ogni preemption ha 2+2 cambi di contesto, salvo e metto quello dell'altro e al ritorno salvo e metto quello del processo precedentemente sospeso.

## `EXEC` Pointer
C'è un puntatore nel Sistema Operativo che punta al processo in stato di esecuzione. Quando avviene un *process switch*, il processo in fase di running diventa *blocked* o *ready*. Se è stato interrotto da un semaforo viene messo in stato di blocked, mentre se ha esaurito il suo quanto di tempo (scheduler round robin) o in seguito all'arrivo di un processo con priorità maggiore, va in stato di *ready*. Punta al PID del processo in esecuzione.
1. Salvataggo dei parametri nello stack (i registri). Questi parametri sono quelli che verranno salvati nel PCB.
2. Cambio in supervisor mode
3. Salvataggio del contesto nel PCB
4. Cambio dello stato del processo (*ready* o *blocked*)
5. Chiamata allo scheduler
6. ...

## Sistemi a Tempo Condiviso
Ciascun processo esegue per un lasso di tempo determinato (ad esempio 10msec). Lo scheduler in questo caso viene detto *round robin* o *time sharing*: ciascun processo ha a disposizione un round. Alla fine del round, il processore viene assegnato ad un altro processo. Se il processo non ha finito viene accodato. Il *timer interrupt* è un cambio di contesto, non solo tra processi diversi ma anche con il sistema operativo. Quando arriva l'interrupt:
1. Si passa a supervisor mode
2. Si salva il CR e l'IP
3. Si salva il contesto
4. Si esegue l'interrupthandler
5. Si chiama lo scheduler

Un cambio di contesto può essere volontario (il processo stesso decide di bloccarsi), come nel caso di una *read*. Una volta che esegue il sistema operativo non è detto che tocchi di nuovo al processo appena sospeso a eseguire: normalmente si invoca lo scheduler. Quando il cambio di contesto non è volontario è normalmente dovuto a un interrupt, come lo scadere di un quanto di tempo o l'arrivo di un processo I/O che sblocca un processo con priorità maggiore.

## Duplice natura del Processo
### Flusso di Esecuzione
Ogni processo nei sistemi multithreading / multiprocessing ha il proprio flusso di esecuzione. Le priorità determinano questi flussi.

### Proprietà delle Risorse (Ownership)
Un processo è proprietario di determinate risorse, come ad esempio dello spazio di indirizzamento.
- Spazio di indirizzamento virtuale
- Immagine del processo (codice + dati)
  
Se più processi vogliono scrivere o leggere contemporaneamente è il sistema operativo che si occupa di arbitrarli.

## Thread vs. Processo

Con il termine `THREAD` si fa riferimento al flusso di esecuzione (si immagini un filo), mentre con il termine `PROCESSO` si fa riferimento al *proprietario delle risorse*. Normalmente si hanno *più flussi di esecuzione* tutti nello stato di memoria appartenente a un processo. I thread prendono anche il nome di *lightweight processes*. Mentre un processo può avere più thread, un thread può appartenere a *un solo processo*.

### Modello Single Thread
A un processo corrisponde un solo thread.
- uno spazio di indirizzi
- uno stack
- un PCB

Ogni processo ha un'*user stack* e un *kernel stack* dedicato per le chiamate di sistema.

### Modello Multithread
Ogni processo può avere più thread:
- uno spazio di indirizzamento
- un PCB ma multipli TCB (thread control block)
- più stacks
- i thread sono gestiti da uno scheduler globale

Ogni thread ha il suo stack composto dalla parte user e dalla parte kernel. La parte di memoria stack è privata per ciascun thread. La memoria globale (*heap*), può essere invece condivisa da più flussi di esecuzione. I flussi in questo modo possono leggersi e scriversi i dati condivisi. Processi differenti possono condividersi variabili globali solo attraverso *chiamate di sistema*. Si ha un incremento delle prestazioni con i thread: più veloci, d'altro canto dal punto di vista della sicurezza è peggio. Un thread che da problemi potrebbe portare i suoi problemi anche agli altri. Anche i *thread context switch* sono nettamente più veloci rispetto a un cambiamento di processo. I processi sono per lo più utilizzati per *competere* per l'ottenimento delle risorse, mentre dei thread vengono utilizzati per *collaborare* al fine di raggiungere un obiettivo.

#### Esempio di un Applicazione Word
- Attesa dell'Input dell'utente
- Aggiornamento del Documento
- Formattazione del Documento
- Controllo errori di sintassi
- Controllo altri eventi (salvataggio automatico)

In questo caso l'utilizzo di un singolo processo sarebbe uno spreco di tempo. Una soluzione potrebbe essere quella di sfruttare i tempi di attesa (che rappresenta la maggior parte del tempo). Mentre si aspetta un task sarebbe bene portarsi avanti con gli altri. Se lo dividessimo su più processi ci sarebbe la problematica di interfacciarsi su un unico documento quando hanno tutti spazi di indirizzamento separati. Solitamente si costruisce un processo owner che funziona da server: è proprietario del documento e gli altri per interfacciarsi ad esso lo interrogano. Questo approccio protegge in caso di funzionamenti ma è una soluzione *lenta*: troppe chiamate di sistema. La soluzione ideale consiste nell'utilizzo di un unico processo e più threads. Unic processo proprietario della risorsa documento e diversi core per i task di input, format, sintassi, grafica. La concorrenza aiuta non solo sulla divisione su più core ma anche sul singolo core. Quando uno di questi thread è bloccato gli altri possono proseguire.

Lo scheduler gestisce i flussi di esecuzione (i thread): è importante ricordare che il processo è il caso degenere con un solo thread.

# Concorrenza
Con il termine si fa riferimento alla collaborazione di thread che concorrono per lo stesso obiettivo. Il processore non è mai in stato IDLE e viene sempre assegnato a qualcuno in attesa. la concorrenza può essere:
- **COMPETIZIONE**: un server è proprietario di risorse e ne regola l'accesso mediante determinate API. Le attività di competizione devono essere *protette*. Se un processo deve accedere alla risorsa, manda una richiesta al server. Le azioni sono regolate dalle due principali primitive `send` e `receive`. La competizione viene implementata normalmente mediante *message passing*. Il message passing è molto semplice: si chiama una send o una receive.
- **COOPERAZIONE**: l'allocazione della risorsa non è più centralizzata. Ogni thread può accedere alla risorsa: qui i thread possono cambiarsi il terreno sotto ai piedi a vicenda (corse critiche, accesso sincronizzato, mutua esclusione). la responsabilità della correttezza del programma non è demandata a un server centrale, ma a ciascun thread. Un thread che accede a una sezione critica *deve sempre proteggerla*. La cooperazione viene implementata normalmente mediante *shared memory*. La shared memory è difficile da implementare, ma porta a un incremento non indifferente delle prestazioni.

## Shared Memory
In generale è possibile allocare una risorsa *staticamente* o *dinamicamente*. Se statica una volta assegnata una risorsa a un processo non può essere revocata. L'accesso può essere dedicato (a un singolo thread) o condiviso (a più thread).

### Mutua Esclusione
Ogni thread con le sue tempistiche legge e scrive su buffer condivisi. Non sappiamo quando va in esecuzione ciascun thread: è un sistema asincrono.

#### Atomicità
Consente di essere sicuri che nel momento in cui si esegue un'operazione (in scrittura o in lettura) su una risorsa, nessun altro thread può eseguire una qualsiasi altra operazione su di essa. Un'operazione atomica non può essere interrotta.

##### Esempio: Variabile x
Shared Memory
```c
x = x + 1
```
Stack 1
```c
void *thread(void *){
    ...;
    x = x + 1;
    ...;
}
```
Stack 2
```c
void *thread(void *){
    ...;
    x = x + 1;
    ...;
}
```
Cattivo Interlacciamento - Assembly
```assembly
LD R0,x TA  x = 0
LD R0,x TB  x = 0
INC R0  TB  x = 0
ST,x R0 TB  x = 1
INC R0  TA  x = 1
ST,x R0 TA  x = 1
```

# Consistenza
Si tratta di un'espressione booleana su delle variabili. Questa *proprietà di consistenza* deve valere *prima o dopo un'operazione*, non durante l'operazione stessa. Con la gestione della concorrenza (semafori) due operazioni si devono sequenzializzare. La consistenza ha a che vedere con la *correttezza del programma*. Per ogni potenziale ricombinazione sulla risorsa (arrivo dei thread), la consistenza è garantita *dopo ogni operazione*. Dopo un aggiornamento la proprietà di consistenza deve essere garantita.

### Esempio: Array Circolare
Immagazzinare dati che lo scrittore scrive con una sua tempistica, e un lettore legge con le sue tempistiche. Circolare perchè scivo in testa ed estraggo in coda. Algoritmi SLAM: *simultaneous localization and mapping*. QUando l'*head* raggiunge la *tail* l'array è *pieno*. `head++` e `num--` sono *operazioni atomiche*: nulla si deve frapporre nella loro esecuzione.

#### Quando è consistente?
3. Il numero degli elementi presenti nel buffer è dato da: *NUMERO INSERIMENTI - NUMERO ESTRAZIONI*

# Correttezza
Un programma non è corretto se esiste almeno un caso di arrivo dei thread che causano uno stato incosistente.

# Sezioni Critiche
Le parti del codice dove si accede a risorse condivise prendono il nome di *sezioni critiche*. Si tratta di una sequenza di operazioni che non può essere interlacciata con altre.

# Problema Produttore-Consumatore
Il Produttore continua a chiedere al buffer lo spazio per inserire ma senza mai liberare il processore: senza mai quindi dare la possibilità al consumatore di consumare le risorse e quindi liberare spazio all'interno del buffer. Se la coda è piena sarebbe bene che si bloccasse e liberasse il processore in modo da dare spazio ad altro. Ogni thread è responsabile del risveglio di un altro. I semafori hanno un doppio utilizzo:
- mutua esclusione per sezioni critiche 
- sincronizzazione tra thread diversi

# Semaforo
Si tratta di un contatore e una *coda bloccante*. Una coda di task bloccati su un semaforo. E' una struttura dati formata da queste due componenti: un contatore (intero), e una coda. Il semaforo va inizializzato. Ammette due operazioni *wait* per bloccare un task, *signal* per risvegliarlo. Queste due operazioni si possono considerare atomiche. 

## Wait and Signal
La wait blocca il thread e lo mette in coda. 

Se un thread viene messo su una coda generica insieme ad altri, anche se è il primo ad andare in coda non è detto che sia il primo a svegliarsi: per introdurre una priorità è necessario introdurre una coda apposita. Non è possibile sapere chi eseguirà dopo una *signal*. Può eseguire chiunque tra quelli bloccati sul semaforo.

## Sincronizzazione
Se il consumatore arriva prima che il produttore abbia prodotto si blocca. Un semaforo non risolve il caso di *due insert*, ma solo quello di *un insert e di un extractor*. Per risolvere un problema con due thread dello stesso tipo è necessario un meccanismo di *mutua esclusione*.

## Implementazione
Le operazioni su risorse condivise nel codice devono essere atomiche. Se un thread deve agire su una determinata risorsa nessuno si deve frapporre sulla stessa.

### RISC V (Five) "atomic" support
- LOAD RESERVED: fa la load ad un indirizzo e nel frattempo monitora se qualcun altro per un determinato periodo di tempo richiede l'accesso a quell'indirizzo.
- STORE CONDITIONAL: una volta fatta la *load reserved* tengo traccia in scrittura se l'allocazione ha successo o fallisce. In caso di successo in *rd* trovo 0. *rd* va controllato in seguito a una store conditional.

#### Esempio
    Example 1: Atomic swap (to test/set lock variable) again:
    bre lr.d x10,(x20) 
    sc.d x11,x23,(x20) // X11 = status bne 
    bre x11,x0,again // branch if failed 
    addi x23,x10,0 // X23 = loaded value
  Ciòche avevo in x23 lo metto in memoria su x20, ciò che avevo su x20 lo metto su x23. L'atomicità è divisa su 4 operazioni diverse.

    Example 2: Lock 
               addi x12,x0,1 // copy locked value
    again: lr.d x10,(x20) // read lock
           bne x10,(x20)  // check if it is 0 yet
           sc.d x11,x12,(x20) // attempt to store
           bne x11,x0,again // branch if it fails
    -   unlock:
           sd    x0,0(x20) // free lock
           
  Finchè x10 è diverso da 0 lo rileggo (lock occupato). Continua a leggere e a riservare spin. Quando lo trova libero lo riserva. Si tratta di una store conditional: altri 47 core stanno facendo lo stesso spinning, e se qualcun altro legge zero nello stesso momento assume di poter entrare. Per questo non si utilizza una store normale ma una store conditional. Se il valore ottenuto dalla store conditional è diverso dallo zero (che è il successo), ritorna all'again.

  Queste tipo di operazioni assembly vengono fatte in *kernel space*. Lo user space non può scrivere liberamente nelle aree di memoria. E' richiesto un cambio di contesto.

# Monitors
Il concetto di Monitors o *condition variable* nasce per consentire di risvegliare più task in attesa. Un monitor è un oggetto che fa in modo che solo *un thread alla volta possa chiamare le proprie funzioni*, e quindi possa accedervi. Se due thread cercano di accedere allo stesso oggetto di tipo monitor, uno dei due viene bloccato. Fornisce una *mutua esclusione implicita*: i semafori sono inseriti implicitamente dal compilatore. Supportano la *sincronizzazione* mediante delle *condition variables*, che si può immaginare come una *coda dei task bloccati*. Il thread si blocca da solo mediante una *cond_wait()* (non per un semaforo), e viene risvegliato mediante una *cond_signal()* da un altro thread, questa funzione non incrementa alcun contatore: si limita a risvegliare un thread se questo è presente tra quelli in attesa, altrimenti non fa nulla.

## Condition Variables
Quando un thread si blocca su una condition variable, la mutua esclusione viene rilasciata in favore di altri thread. Quando un thread viene riscegliato da qualcuno, deve ancora controllare di poter eseguire. Per questo è bene mettere il controllo all'interno di un ciclo *while*.
- `pthread_mutex_lock(&ca->mymutex);` è l'equivalente di pthread_wait_mutex
- `thread_cond_wait(&ca->full,&ca->mymutex);` sintetizza il bloccaggio del mutex, il bloccaggio su una *condition variable* e il rilascio del mutex
- `thread_cond_signal(&ca->full,&ca->mymutex);` non è bloccante, pertanto non ha bisogno del mutex

# Message Passing
Esistono due tipi di operazioni: *send* e *receive*.
La *send* può essere *sincrona o asincrona*, mentre la *receive* può essere *simmetrica o asimettrica*. Entrambe queste primitive si basano sul concetto di messaggio: cosa si vuole mandare e a chi. Il messaggio può anche essere una struttura dati. Quando si scrive una *send* si può scegliere se aspettare la conferma di ricezione oppure se continuare ad eseguire anche senza conoscere l'esito dell'operazione. In alcuni casi può essere necessario un'operazione sincrona. Tuttavia, a volte, trattandosi per lo più di task periodici, si potrebbero utilizzare gli ultimi risultati ottenuti per proeguire nell'esecuzione. La receive deve per forza aspettare che arrivi un dato: è pertanto sempre sincrona.

  **Esempio: Problema Produttore / Consumatore:**

    Lo stesso problema precedentemente affrontato è meno performante, ma più semplice. Non sono necessari mutex o semafori: queste operazioni non hanno bisogno di protezioni, queste sono già presenti e nascoste dall'interfaccia che la *send* e la *receive* espongono.

## Risorse e Scambio di Messaggi
Non ci sono risorse condivise. La risorsa è gestita da un *processo gestore*, che è l'unico in grado di *accedere alla risorsa*. Non ci si deve preoccupare pertanto di proteggere le risorse condivise che sono accedute da più thread: il processo gestore è il *proprietario della risorsa*, se gli altri vogliono accedervi lo fanno mediante delle *send o delle receive al processo gestore*.

### Esempio: Comunicazione Sincrona
Si vuole capire cosa succede se arriva prima la send o la receive. 2 Processi: Ricevitore e Produttore. Dopo una *send sincrona* il processo produttore *si blocca*. Quando il ricevitore esegue la receive manda l'ack al produttore che in questo modo si può svegliare. Se esegue prima il ricevitore, questo si blocca immediatamente se in determinato lasso di tempo non riceve alcun messaggio, permettendo al produttore di eseguire.

### Esempio: Comunicazione Asincrona
La send può essere asincrona: perchè questo sia possibile deve essere presente un *send buffer* per immagazzinare il dato e far sì che non venga perduto. La receive rimane sempre sincrona: anche in questo caso se non riceve nulla si mette in attesa di un messaggio, permettendo al produttore di eseguire e di inviare un messaggio. Sarà il produttore con una sorta di signal a svegliarlo.

## Receive Simmetrica e Asimettrica
Il ricevitore può discriminare se vuole un messaggio da una particolare sorgente o da chiunque: nel primo caso la receive è simettrica, mentre nel secondo caso è asimettrica. Se con una receive simettrica si riceve il messaggio da una sorgente diversa da quella desiderata, il messaggio viene ignorato.
```c
receive(source, &data) // Simmetrica
receive($data)         // Asimettrica 
```

## Remote Procedure Call (RPC)
Caso in cui non vengono inviati dei messaggi, ma dei *compiti che devono essere eseguiti*. Solitamente utilizzato in uno scenario *client-server* (richieste Google, ad esempio): una volta inviata la richiesta il client si mette in attesa di una risposta.

Nello scambio di messaggi ogni risorsa ha una thread manager che è responsabile di fornire accesso alla risorsa. un thread che vuole accedere alla risorsa manda un messaggio al thread gestore per richiedere l'accesso alla sezione critica, e lo notifica quando vi sta per uscire. Il thread gestore si può considerare una sorta di *arbitro* che deve assicurare la mutua esclusione tra *thread A* e *thread B*. Il processore gestore si mette da prima in attesa di un thread qualunque che richiede l'accesso alla risorsa, una volta fornito l'accesso si mette in attesa di un altro dato, stavolta specificando la sorgente da cui si aspetta il messaggio, ovvero il thread che prima ha richiesto l'accesso e che una volta terminata la sezione critica deve notificare il gestore.

## Send e Receive Sincrone
L'utilizzo dei samafori si ha un'implementazione più *lightweight*: più personalizzabile: lo scambio di messaggi ci vincola sempre all'utilizzo di un thread gestore. I thread che si devono scambiare messaggi utilizzano le primitive send e receive dove la mutua esclusione è già implementata, utilizzando i semafori è possibile programmare la mutua esclusione stessa.

## Send Asincrona e Receive Sincrona
Il manager si mette sempre in attessa di un messaggio da un thread qualunque. Dopo che un thread richiede la risorsa, può andare avanti nell'esecuzione (aspettando ad esempio di ricevere il dato, mettendosi in attesa). Il gestore fa una send asincrona fornendo al thread la risorsa desiderata, che in questo modo può sbloccarsi dall'attesa. Una volta uscito dalla sezione critica, il thread che ha richiesto l'accesso manda una send asincrona al manager che si aspetta una risposta proprio da lui.

# Deadlocks
Situazione in cui un gruppo di thread è bloccato permanentemente, ciascuno in attesa di una risorsa o che venga compiuta un'azione da un altro thread. Un thread in una situazione di deadlock tenta continuamente di accedere a una risorsa ma la sua richiesta non viene mai soddisfatta. Il *Livelock* è un *caso particolare* del deadlock che si verifica quando un thread continua ad eseguire *richiedendo l'accesso ad una risorsa e continuando a ricevere un esito negativo*. Una configurazione che porta al deadlock non è detto che con una seconda esecuzione riporti ad una situazione di deadlock, talvolta può trattarsi di un'esecuzione sfortunata, ma è bene evitarla considerando i diversi scenari di esecuzione dei thread.

## Risorse Consumabili e Riutilizzabili
Una risorsa *riutilizzabile* è utilizzabile da *un thread alla volta*, ma che non viene consumata dal suo utilizzo. Un thread che utilizza questa risorsa una volta finito di utilizzarla la mette a disposizione di altri thread. Le risorse *consumabili* non sono più utilizzabili da altri thread dopo che vengono utilizzate da un thread. I deadlock si verificano solitamente con *risorse riutilizzabili*. I deadlock si possono verificare anche senza l'utilizzo di semafori: si pensi ad esempio alla richiesta di allocazione di memoria su un sistema embedded con una memoria limitata (ad esempio 200kb): due thread A e B fanno diverse richieste, ciascuno fino a un massimo 150kb, se però la memoria non viene liberata per tempo i due thread rimangono in attesa di una memoria che non è disponibile. Se queste richieste vengono implementate all'interno di un *ciclo while* i thread continuano a chiedere la risorsa memoria, pertanto si tratterebbe di un caso di *Livelock*.

## Condizioni per il Deadlock
Gli "ingredienti" necessari per causare un deadlock sono *tre*, e sono necessari *tutti* per causarne uno:
- Allocazione dinamica di una risorsa dedicata: c'è un concetto di esclusività legato ad una determinata risorsa. Solo un processo alla volta la può utilizzare.
- Trattieni e aspetta: un processo potrebbe *trattenere una risorsa* quando si mette *in attesa per riceverne un'altra*.
- Assenza di preemption: la risorsa non viene revocata al thread da cui viene trattenuta.

## Soluzione al Problema del Deadlock
L'idea alla base di una possibile soluzione consiste nell'evitare che una delle condizioni precedenti sia verificata. E' possibile distinguere due tecniche:
- Statica: si fa in modo che ci siano regole 

Ci sono 3 diverse strategie:
- Deadlock Prevention (Static)
- Deadlock Avoidance (Dynamic)
- Deadlock Detection (Dynamic): attuabile solo *dopo il verificarsi del deadlock*. Consiste nel revocare la risorsa al thread che la detiene, è sempre un'azione pericolosa che è meglio evitare.

### Deadlock Prevention
- Consentire a un thread di prendere tutte le risorse nello stesso momento

#### Condizioni
- Hold and Wait: Possiamo imporre a un task di prendere tutte le risorse di cui ha bisogno nel momento in cui ne ha bisogno. Quest'azione è restrittiva, in quanto si va a limitare di molto, se non a eliminare quasi completamente la *concorrenza*.
- No Preemption: questa tecnica è attuabile solo se è possibile sospendere l'esecuzione di un thread in favore dell'esecuzione di un altro thread. Se la risorsa condivisa è il processore, quest'azione è attuabile mediante un *cambio di contesto*, tuttavita non tutte le risorse possono essere sottratte a un thread in fase di esecuzione.
- Circular Wait: condizone che si può evitare definendo un'*ordine lineare di accesso alle risorse*.

Rappresentando la situazione attraverso dei grafi, si utilizza la seguente metodologia di rappresentazione:
- Nodi Circolari : Thread
- Nodi Rettangolari: Risorse
- Archi: Richieste delle Risorse
Si può verificare una condizione di deadlock se nel grafo sono presenti dei cicli.

## Stretegie di Recupero
1. Eliminare tutti i thread
2. Check Point: ogni thread salva un punto sicuro nel quale il sistema operativo può tornare in caso di deadlock.
3. Eliminare un thread alla volta, fino a quando il deadlock scompare
4. Togliere la risorsa al thread in deadlock
