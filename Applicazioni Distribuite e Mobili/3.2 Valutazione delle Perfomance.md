- [Valutazione delle Performance](#valutazione-delle-performance)
  - [Parametri di Performance](#parametri-di-performance)
    - [Capacità di Pianificazione](#capacità-di-pianificazione)
  - [Testing di Sistemi Reali](#testing-di-sistemi-reali)
    - [Tipologie di Benchmark](#tipologie-di-benchmark)
      - [Benchmark Standards](#benchmark-standards)
        - [SPEC Web](#spec-web)
        - [TPC](#tpc)
        - [SPEC-Cloud](#spec-cloud)
        - [YCSB](#ycsb)
        - [PerfKit](#perfkit)
  - [Modelli Matematici](#modelli-matematici)
    - [Sistema a Coda](#sistema-a-coda)
      - [Legge dell'Utilizzazione](#legge-dellutilizzazione)
      - [Legge di Little](#legge-di-little)
      - [Modelli ad Anello](#modelli-ad-anello)
    - [Teoria delle Code](#teoria-delle-code)
      - [Processo Poissoniano `M/M/1`](#processo-poissoniano-mm1)
      - [Altre Formule Utili](#altre-formule-utili)

# Valutazione delle Performance

Per valutare le performance di un sistema è possibile seguire due diversi approcci: fare degli esperimenti sul **sistema reale**, oppure utilizzare un **modello del sistema**. Fare esperimenti sul sistema reale non è mai la via migliore per questioni di complessità e costi: non è pensabile mettere offline un sistema in produzione per delle ore (o giorni). Alcuni test devono essere svolti *senza un sistema reale*, specialmente in ***fase di progettazione***: quando lavorare sul sistema reale non è fattibile si utilizza un **modello in scala del sistema reale**.

Un modello in scala può essere costituito anche da un *rack interno al datacenter*: in questo caso si tratta di un **modello fisico**. Utilizzando sistemi di qualche ordine di grandezza più piccoli di quello reale però non si ha la garanzia che quando sarà necessario il sistema si comporti come deve: un modello fisico è indicato per tutti quegli esperimenti che non è fattibile fare su un sistema reale, ma non è adatto a tutte quelle situazioni in cui è necessaria una *scalabilità estrema*: si tratta di un *approccio costoso*. Su un sistema fisico possono inoltre essere presenti *elementi terzi* che vanno ad invalidare gli esperimenti effettuati, per questo è preferibile talvolta utilizzare un **modello matematico**: un sistema *completamente controllabile* che diventa un ***modello concettuale***. 

Un modello concettuale si dice **puramente matematico** quando il *comportamento e le sue prestazioni* sono *descritte completamente da **equazioni***. L'utilizzo di equazioni permette un ***grande livello di semplificazione***, in quanto permette di rilassare tanti dettagli: si tratta di un ***sistema tanto efficace*** quanto ***poco flessibile***. Un'approccio sempre matematico che conferisce ***più flessibilità*** è quello della **simulazione**: il *comportamento del sistema* in questo caso viene *descritto sotto forma di **codice***. si scrive un programma (o software) che cattura determinati comportamenti del sistema e va a misurare una serie di informazioni. Con la scrittura di codice che simula il comportamento del sistema, andare ad esplorare anche un *numero elevato di soluzioni* non diventa più proibitivo. Inoltre, con un modello matematico è possibile anche utilizzare delle ***funzioni obiettivo per problemi di utilizzazione*** per trovare le configurazioni ideali adatte al proprio scopo (es. minimizzare il tempo di risposta).

```mermaid
graph TD
A[Valutazione del Sistema] -->B[Sistema Reale]
A -->C[Modello in Scala Sistema Reale]
	C -->D[Modello Fisico]
	C -->E[Modello Concettuale - Matematico]
		E -->F[Puramente Matematico - Analitico]
		E -->G[Simulazione]
```

Non esistono approcci giusti o sbagliati, tuttavia è spesso preferibile lavorare su un modello piuttosto che sul sistema reale. Per scegliere tra un modello fisico e uno matematico è necessario scegliere tra l'accuratezza delle soluzioni e la velocità con cui è possibile trovare una risposta: anche il livello di dettaglio è differente: è importante che il **modello matematico** sia quanto ***più accurato possibile*** e *non troppo approssimativo*. Un **modello analitico** risulta ***molto preciso***, mentre una **simulazione** risulta ***più flessibile***.

## Parametri di Performance

- **TEMPO DI RISPOSTA**: tempo che intercorre tra *quando inizia una richiesta* e quando si *riceve l'ultimo byte della risposta*. In questi casi il tempo totale è costituito dal ***tempo legato alla rete*** e dal ***tempo legato al processing***.
- **THORUGHPUT**: *numero di richieste che è possibile soddisfare in una determinata unità di tempo*. Questo parametro è spesso *di interesse per gli amministratori di sistema*.
- **ERROR RATE**: *numero di richieste che non vengono soddisfatte a causa di **overload***.

> <img src="resources/03_01_parametri_performance.PNG" style="zoom: 67%;" />
>
> L'immagine mostra le diverse metriche al variare del carico offerto. In tutti i grafici si riconoscono ***3 macro aree***:
>
> - **Area VERDE**: il ***sistema*** è ***scarico***. Quest'area non presenta errori: le richieste vengono sempre soddisfatte.
>   - Il **`tempo di risposta`** rimane abbastanza contenuto: è probabile che nel momento in cui si fa una richiesta non ci siano altre richieste che entrino in contesa per l'utilizzo delle risorse: non è particolarmente correlata al carico.
>   - Il **`throughput`** aumenta in maniera *lineare*: ottenendo 10 richieste al secondo si hanno 10 risposte al secondo.
> - **Area ARANCIONE**: il livello di carico aumenta e diventa tale per cui si verifica una ***contesa nell'utilizzo delle risorse***. La probabilità che le richieste debbano essere temporaneamente sospese aumenta, per questo motivo aumenta il **`tempo di risposta`**. Nel **`througput`** non si osserva più un aumento lineare del carico, la *curva si appiattisce* perché si stanno *saturando le risorse*: è presente una risorsa che fa da collo di bottiglia ed è già usata, se al sistema si chiede di fare qualcosa in più questo avrà un impatto solo sul tempo di risposta.
> - **Area ROSSA**: si hanno tempi di attesa non più accettabile e nel server cominciano a verificarsi degli errori. Qui il **`tempo di risposta`** comincia a saturare perché è dominato dalla *presenza del time-out*.
>
> L'**obiettivo** del sistema è quello di ***lavorare nella parte terminale della zona verde***: dove le risorse sono ben utilizzate e non si verifica nessuna situazione di *degrado prestazionale*.

### Capacità di Pianificazione

Normalmente un *server mal configurato* presenta *prestazioni scarse* (degrado anche fino al $-60\%$), altri possono comunque funzionare bene ma rispetto a un sistema ben configurato si potrebbe spendere $10\text{x}$ volte di più. La sfida consiste nel ***prevedere il comportamento del sistema*** in modo da **agire in maniera proattiva**. Il **Capacity Planning** consiste nel capire *come scalare il sistema* in modo da spostare la curva di carico in maniera corretta, senza entrare nella zona di soglia. L'**elasticità** è una caratteristica chiave del *cloud computing*, che permette di acquistare *potenza computazionale* aggiuntiva in caso di bisogno. Per acquistare potenza computazionale bisogna innanzi tutto **conoscere le performance del proprio sistema**: di *quante macchine si ha bisogno*, di *che tipo*, *per quanto tempo* e *quali risorse utilizzano*.

Se in una rete fog si vuole conoscere quali nodi accendere e/o spegnere in base al numero di sensori che producono dati, quando si chiede un'elaborazione a un nodo questo ha un ***tempo di servizio*** che segue una certa ***funzione di densità di probabilità***. Per semplificare il problema, si possono utilizzare dei **modelli per descrivere il comportamento del sistema**: questi modelli sono *utili*, ma *non particolarmente corretti*. C'è il rischio che il modello analitico rappresenti una ***situazione troppo semplificativa***: se la funzione di densità di probabilità è caratterizzata da un'alta varianza, allora il tempo di attesa in coda per il modello più semplice viene sottostimato, pertanto in uno scenario reale si potrebbero sforare i SLA. Per rimediare a questi casi è possibile utilizzare le **formule di Pollaczek-Khinchin**, che anticipano lo scaling che nel modello di riferimento utilizzato avviene troppo tardi.

<center><img src="resources/03_02_processo_markoviano.PNG" style="zoom: 80%;" /><img src="resources/03_03_pollaczek_kinchin.PNG" alt="03_03_pollaczek_kinchin" style="zoom: 80%;" /></center>

## Testing di Sistemi Reali

Per quanto riguarda i **modelli fisici**, esistono diverse *tipologie di test*, a seconda dell'obiettivo dell'analisi:

- **FUNCTIONAL** &rarr; si controlla se ***il sistema si comporta come dovrebbe***. Non si guardano le prestazioni del sistema, ma si è più vicino a un ***integration testing***.

- **ACTIVITY** &rarr; di definisce un ***carico di lavoro realistico*** (in termini di numero di richieste) e si guarda come lavora il sistema a fronte di un certo carico nominale. Si guarda lo *scenario normale*. Viene spesso utilizzato per fare dei ***deployment controllati***.

- **ENDURANCE** &rarr; viene definito un ***carico di lavoro più intenso*** (ancora verosimile), come può essere il carico di lavoro durante l'ora di picco. Si guarda la ***capacità di scaling del sistema***. Con questo test *non si vogliono testare i limiti del sistema*, e quindi non lo si vuole rompere: il sistema deve essere in grado di fronteggiare correttamente il carico offerto.

- **STRESS** &rarr; si ricercano i ***limiti del sistema***: si vuole guardare fino a dove si può arrivare senza causare dei problemi. Il carico viene aumentato in maniera controllata fino anche a rompere il sistema. Con questo test è possibile conoscere: 

  - il *limite oltre il quale non andare* 
  - *dove si trova il collo di bottiglia* (ciò che si sta rompendo)

- **BENCHMARK** &rarr; definizione di un ***carico standardizzato*** per avere un'***indicazione delle performance del sistema***: serve generalmente per *fare un **confronto tra due diverse architetture*** (deployment diversi). Si tratta di un *insieme di criteri*, un riferimento che serve per effettuare delle *misurazioni*. L'**obiettivo** è sempre quello di ***definire un workload standardizzato*** per un *confronto*. Il concetto di benchmark risale alla valutazione di sistemi hardware, in particolare i primi ad interessarsene sono stati i *produttori dei sistemi di calcolo*.

### Tipologie di Benchmark

  - **`TOY BENCHMARK`**: si tratta di ***algoritmi***. Vengono utilizzati specialmente quando si vogliono valutare le *prestazioni di un sistema di calcolo* in termini di *processore* (*quicksort*, *operazioni matriciali*).

  - **`PROGRAMMI REALI`**: programmi tendenzialmente CPU-bound (es. *software di ray tracing*, *compilazione del kernel-linux* con le dovute attenzioni alla versione di `gcc` utilizzata, deve essere quanto più standard possibile).

  - **`BENCHMARK SINTETICI`**: partono da alcune *operazioni comuni in molti software mirati* per estrarre frammenti di codice significativi. Due esempi di software sono *Dhystone* e *Whetstone*. I principali ***sponsor dei benchmark*** sono enti che hanno collaborazioni con i produttori di chip. Alcuni dei produttori non mirano a fare la CPU buona in generale, ma ne massimizzano i risultati ottenuti sui benchmark.

  - **`BENCHMARK BASATI SU APPLICAZIONI REALI`**: tendenza attuale volta a evitare le "cattive pratiche" precedenti, che non si basa più su frammenti di codice estrapolati dal contesto. Consiste nella creazione di un benchmark il più possibile ***rappresentativo di un caso reale***. In questi casi i ***test*** sono ***ben documentati***. Viene utilizzata l'intera applicazione, non più solo un algoritmo: il problema affrontato è rappresentativo e si cerca di standardizzare ogni elemento rilevato. Ci sono **enti terzi indipendenti** che negli anni hanno proposto diversi standard di benchmark.

#### Benchmark Standards

##### SPEC Web

Sviluppato da *SPEC* (System Performance Evaluation Consortium). Si occupa di **applicazioni web** prendendo esempio da *tracce di workload il più realistici possibili*. Definisce in ottica web diversi tipi di files di dimensioni diverse, cercando di capire come è costruito il path dell'URL e di modellare il comportamento degli utenti (richiesta e attesa del caricamento di una pagina). Definisce inoltre un'*architettura* con un controller che pilota tanti client a fare richieste verso il server. Negli ultimi anni il sistema di benchmarking si è evoluto con il *supporto per le richieste persistenti* e l'*emulazione della presenza dei cookies*.
##### TPC

Esistono diverse varianti: ***TPC-W*** peri benchmark *web-based*, ***TPC-D*** per i *database*. Con questo sistema si cerca di ***creare un workload basato su un'applicazione***: nella sua prima versione TPC-W creava un *book store di aste online* molto simile ad *Ebay*, poi è stato cambiato in modo che si rifacesse ad *Amazon*. Anche in questo sistema è presente una parte di codice web, e una di comportamento degli utenti (*browsing di prodotti*, *acquisto*). Viene definito in modo preciso il **comportamento degli utenti** andando a creare una ***matrice di probabilità*** che indica la *probabilità di passare da una pagina a un'altra*: i nodi sono le pagine del sito e gli archi la probabilità di passare tra una pagina e l'altra. Cambiando i pesi degli archi è possibile definire *workload diversi*, per descrivere contesti differenti.

##### SPEC-Cloud

Orientato a sistemi cloud. Testa la ***scalabilità*** (incremento del carico senza sfociare in errori) e l'***elasticità*** (variare il carico mantendendo le SLA) di un cloud provider.

##### YCSB

Benchmark di riferimento per sistemi distribuiti: viene utilizzato per testare le prestazioni di un database NoSQL (Cassandra), prevalentemente utilizzato da Facebook nelle sue applicazioni. Tipicamente come *metrica* si guarda il ***throughput*** e come cambia al variare del numero di nodi.

##### PerfKit 

Tool di Google che ***valuta soluzioni Cloud diverse***. Dispone di un motore per creare ed eliminare automaticamente un certo numero di nodi, supporta il deployment sia sul Cloud Google, che su sistemi basati su Open Stack e il deployment fatto su ambienti containerizzati (es. *Docker*): inoltre supporta una discreta collezione di workload che possono essere utilizzati. Ha una struttura molto complessa.

## Modelli Matematici

Ci concentriamo sui Modelli *puramente matematici* (noti anche come *soluzioni analitiche*), *non sulle simulazioni*.

Per capire come sono fatti questi sistemi, è possibile prendere d'esempio un ***Sistema Web Multi-Tier***, composto da:

- ***front-end***
- ***back-end***
- ***middle-tier***
- ***rete*** (che interconnette i *tier* tra di loro)

Tutti questi componenti comportano un ***contributo al tempo di risposta***. In particolare, il tempo di risposta di ciascuno di questi server (nodi) è dato da altri elementi:

- CPU &rarr; tempo speso a fare operazioni di calcolo
- Storage &rarr; tempo di risposta che varia dal tipo di disco utilizzato
- LAN &rarr; tempo legato alla comunicazione nella rete

È possibile associare i singoli contributi di questi tempi all'***utilizzo di una risorsa***. Il modello matematico più comodo per descrivere l'utilizzo delle risorse deriva dalla **Teoria delle Reti di Code** (anni $'60/'70$), utilizzato ***per descrivere la rete***. L'input nel caso della rete è dato dall'arrivo delle richieste, che possono essere accodate nel buffer della scheda di rete. Quando il canale trasmissivo (risorsa) è libero, allora la scheda di rete lo occupa per il tempo necessario a trasferire il pacchetto. Se il canale trasmissivo è occupato, allora il pacchetto rimane in coda fino a quando la risorsa non viene liberata. Con questo *approccio* è possibile *descrivere l'uso di una risorsa limitata*.

Un generico **server** dunque avrà un ***modello a coda per ciascuna risorsa***: la CPU, il disco e la scheda di rete. Il tempo di risposta del server è dato dal contributo di ciascuna di queste risorse. Nel **sistema web**, è possibile prendere ciascun server per creare un sistema più complesso in modo che ci siano $n$ server per il *backend*, $n$ server per il frontend, $n$ server per il middle-tier. Il modello creato rappresenta sicuramente una semplificazione del sistema reale, ma è una buona approssimazione che consente di ***conoscerne i limiti e le prestazioni*** (es. se il problema risulta essere nel disco, è possibile mettere un raid di dischi in modo da parallelizzare la risorsa).

### Sistema a Coda

Il problema viene modellato utilizzando un **sistema a coda**. Di seguito vengono presentate alcune grandezze e strumenti matematici utili a misurare le *prestazioni di un singolo nodo*.

- WORKLOAD

  - $S_i$ : **tempo di servizio sulla risorsa** $i$-esima. Nei sistemi $MM1$: $S_i = \frac{1}{\mu_i}$ ***inverso del tasso di servizio (throughput)***

  - $W_i$ : **tempo di attesa sulla coda** $i$-esima

  - $\lambda_i$ : **tasso di arrivo** (o ***carico offerto***) alla coda $i$-esima

- METRICHE DI PERFORMANCE

  - $R_i$ : **tempo di risposta** alla coda $i$-esima
  - $X_i$ : **throughput** della coda $i$-esima
  - $X_0$ : **throughput complessivo** di più elementi

- NUMERO DI JOBS (RICHIESTE)

  - $N_{iw}$ : **numero di jobs in attesa** nella coda della risorsa $i$-esima
  - $N_{is}$ : **numero di jobs in fase di servizio** compreso tra $[0,m]$  dove $m= \text{numero di servitori}$ nella risorsa $i$-esima
  - $N_i$ : **numero di jobs all'interno dell'$i$-esima risorsa**

#### Legge dell'Utilizzazione

L'**utilizzazione** si definisce come il *tempo* $B_i$ *in cui il server è occupato in un determinato periodo* $\tau$. Introducendo i parametri:

- $B_i$ : **tempo in cui il server è occupato**

- $\tau$ : **periodo di tempo** per il quale si osserva il sistema

quindi si ha che l'utilizzazione $U_i=\frac{B_i}{\tau}$. Definendo in seguito:

- $C_i$ : numero di **richieste completate** nel periodo di tempo

È possibile notare che il **throughput** è dato dal *numero di richieste completate* $C_i$ *nel periodo di tempo* $\tau$, pertanto $X_i = \frac{C_i}{\tau}$. Riscrivendo $\frac{X_i}{C_i} = \frac{1}{\tau}$ si ottiene la **frequenza di campionamento del sistema**. Facendo una sostituzione, è possibile ottenere l'utilizzazione come:
$$
U_i = B_i \cdot \frac{X_i}{C_i} = \frac{B_i}{C_i} \cdot X_i = S_i \cdot X_i \\
\text{in uno stato stabile (concentrandosi solo sui valori medi)} \\
U_i = S_i \cdot \lambda_i
$$
Dove $\lambda_i$ è il ***tasso di arrivo delle richieste***. Spesso l'utilizzazione si definisce anche come: $\rho = \frac{\lambda}{\mu}$.

Volendo estendere il tutto ad un contesto multicoda, considerando il servizio suddiviso su $m$ servitori differenti si ha:
$$
U_i = \frac{S_i}{m} \cdot X_i
$$

#### Legge di Little

Si assume il sistema come una black box: non si guarda più il numero di servitori o le code. Si considera solamente:

- $N$ : numero di ***richieste nel sistema***
- $X$ : ***throughput***
- $R$ : ***tempo medio delle richieste*** all'interno del sistema

La notazione di Little afferma che **il numero di richieste** all'interno del sistema è:
$$
N = X \cdot R
$$
È possibile applicare la Legge di Little ai *singoli componenti del sistema*:
$$
N_{iw} = X_i \cdot W_i \qquad N_{is} = X_i \cdot S_i \qquad N_i = X_i \cdot R_i
$$
Dove $N_{is}$ è il ***numero di jobs in uno stato di servizio***, che ***corrisponde all'utilizzazione***. 

#### Modelli ad Anello

I sistemi a coda possono essere modellati secondo due diversi approcci:

- **ANELLO APERTO**

  Il ***numero di richieste all'interno del sistema è potenzialmente infinito***: non vengono fatte assunzioni. Le richieste entrano nel sistema, vengono processate ed escono. Non ci sono vincoli, se non quelle riguardanti le proprietà del processo di arrivo delle richieste. Un sistema di questo tipo mano a mano che vengono fatte delle nuove richieste crea nuovi thread con una certa probabilità.

- **ANELLO CHIUSO**

  La ***popolazione di utenti*** (richieste) all'interno del sistema è ***finita***. Tipico scenario web: alcuni utenti fanno le richieste, altri si mettono in attesa. La **Legge di Little per un Sistema Chiuso**, dati i seguenti parametri, afferma:

  - $M$ : numero di utenti nel sistema
  - X : throughput
  - $R$ : tempo di risposta
  - $Z$ : tempo di servizio

$$
R + Z = \frac{M}{X}
$$

### Teoria delle Code

Ciò che avviene all'interno del sistema viene descritto mediante un **processo stocastico**, ovvero sulla base di un *comportamento casuale* descritto esclusivamente in *termini statistici*. A seconda delle caratteristiche di questo comportamento è possibile definire *famiglie diverse di sistemi a coda* che hanno *leggi descrittive differenti*. In generale, una variabile aleatoria con quelle che sono le sue caratteristiche statistiche. La caratteristiche che si utilizza più spesso è la **funzione di densità di probabilità**.

Si assumono ***sistemi ad anello aperto***. Con i sistemi a coda si hanno due elementi fondamentali che caratterizzano il comportamento del processo statistico:

- **processo di arrivo delle richieste **&rarr; *legge* che descrive la variabile aleatoria rappresentante il ***tempo di inter arrivo tra una richiesta e l'altra***
- **processo di servizio delle richieste** &rarr; *legge* che descrive la variabile aleatoria rappresentante il ***tempo per servire una determinata richiesta***

In generale la notazione utilizzata è la seguente: `{Processo di Arrivo}/{Processo di Servizio}/{#Servitori}/{Dim. Coda}`

Se la lunghezza della coda non viene specificata si assume che abbia *lunghezza infinita*. I Processi possono essere classificati con delle lettere, ciascuna corrispondente a una ***distribuzione di probabilità***:

- **`M`** &rarr; Processo di *Poisson* (senza memoria). ***Processo di Arrivo Markoviano***: in qualsiasi momento può arrivare un nuovo job o finire il servizio di un job.
- **`D`** &rarr; Processo Deterministico: l'arrivo di un job o il tempo di servizio sono *valori certi*.
- $E_k$ &rarr; Distribuzione di *Erlang*
- **`G`** &rarr; Processo *Generico*: non viene fatto alcun tipo di assunzione. Solitamente si utilizzano *distribuzioni Gaussiane*, *log-normali* o *esponenziali*.

#### Processo Poissoniano `M/M/1`

Questo modello non cattura un sistema realistico.

- $\lambda$ : tasso di arrivo
- $\mu$ : tasso di partenza (*ritorno allo stato precedente*)
- $U = \rho = \frac{\lambda}{\mu}$ (*Utilizzazione*)
- $p_k = (1-\rho) \cdot \rho_k$ (*Probabilità di trovarsi nello stato $k$*, ovvero *avere $k$ job all'interno del sistema*)

<img src="resources/03_04_processo_poissoniano.PNG" style="zoom:50%;" />

- $T_{risposta} = \frac{1}{\mu - \lambda}$. Questo *aumenta con l'aumento dell'utilizzazione* (e quindi del carico) del sistema.

#### Altre Formule Utili

Utilizzando l'equazione di **Pollaczek-Khinchin** è possibile descrivere il **processo di servizio** utilizzando due parametri: il ***valore medio*** e la sua ***deviazione standard***. Definiti:

- $E[W]$ : ***tempo di attesa medio***
- $E[T]$ : ***tempo di risposta medio***
- $E[S]$ : ***tempo di servizio medio*** ($\frac{1}{\mu}$)

$$
\begin{cases}
	E[W] = \frac{\lambda E[S^2]}{2(1-\rho)} = \frac{1+C_v^2}{2}\cdot\frac{\rho}{1-\rho}\cdot E[S] \\ \\
	E[T] = E[S] + E[W] = E[S] + \frac{\lambda E[S^2]}{2(1-\rho)} = (1 + \frac{1+C_v^2}{2} \cdot \frac{\rho}{1-\rho}) \cdot E[S]
\end{cases}
$$

Dove:

- $\frac{\rho}{1-\rho}$ : è il ***tempo di attesa di un sistema*** **`M/M/1`**
- $C_v^2$ : è il ***coefficiente di variazione***, ovvero il ***rapporto tra la deviazione standard e il valore medio***. Per *Poisson* il valore di questo coefficiente è $1$.
