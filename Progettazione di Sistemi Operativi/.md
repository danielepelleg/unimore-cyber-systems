SCALE UP il singolo server viene migliorato
Non è capace di tenere il passo con gli incrementi HW e computazionali.

SCALE OUT
singoli processori con numero di processori che cresce pergestire più accessi concorrenziali

## Replicazione del Dato
La replicazione può essere **totale** o **parziale**. La consistenza può essere **forte** o **debole**: con una consistenza forte la modifica di un dato viene propagata immediatamente a tutte le copie del dato, mentre con una debole la consistenza viene garantita ma non immediatamente, possono esserci stati di incosistenza. I sistemi non usano sempre una replica totale con consistenza forte per il problema di **overhead**. Mantenere una consistenza forte con copie distribuite diversamente anche a livello geografico non è una cosa semplice, richiede delle risorse computazionalmente potenti (a partire dalla rete, dalla connettività di internet che è da sempre suscettibile a interruzioni), mantenendo una consistenza ***immediata*** dei dati. A livello atomico, per garantire queste operazioni devono essere utilizzati dei mutex: l'immediatezza non è mai qualcosa di banale, richiede di bloccare tutto.

### Sincronizzazione dei Dati
Posso interrogare un dato su una qualunque delle copie che tanto non fa differenza. Se voglio una strong consistency si ha un overhead talmente elevato da diventare un collo di bottiglia. Questo overhead tende a crescere col numero di copie, comportando una diminuzione della scalabilità. I più grandi sistemi in cloud adottano una soluzione che è una via di mezzo tra strong e weak consistency. Devo trovare un ***compromesso*** pensando alle caratteristiche dell'applicazione.  Si devono inoltre individuare i vincoli di consistenza che si hanno. Il layer di software deve essere trasparente, garantendo l'indipendenza dal tipo di replica del dato alla quale si accede o che si sta utilizzando. A volte la propagazione della sincronizzazione avviene in base all'***area geografica***.

#### Strong Consistency
La modifica comincia dal master e viene propagata sui primary. Se sul primary non si ha un singolo utente ma tanti flussi di transazione. Senza un meccanismo di controllo non è possibile conoscere lo stato consistente del sistema. 

## Sistema di Controllo delle Concorrenza
Le transazioni che arrivano sparse devono essere organizzate in maniera sequenziale. Il **protocollo di consenso** consente alle repliche di mettersi d'accordo sulla stori di eventi accaduti sul sistema distribuito. Il sistema di controllo deve permettere l'operazione **dopo** la sincronizzazione. Un accesso in scrittura viene sempre fatto sul **master**, che non necessariamente è sempre lo stesso, ma al contrario può ruotare. Per gli accessi in lettura non fa differenza. Il master tiene traccia dell'ordine corretto delle operazioni.

Con **one copy seriazability** si ha **high availability**. È un data center primario master che serializza tutte le operazioni in scrittura. 

## Sistema GEO Distribuito
Il Master può essere diverso a seconda dell'account.

## Commit a Due Fasi
Si tratta di un protocollo di consenso ampiamente utilizzato. Nel omento incui un client vuole eseguire una transazione la richiede al suo primary master, il quale ne può ricevere anche più di una e ne comincia a stabilire un ordine. Questo invia una richiesta di *prepare to commit* a se stesso e agli altri primary. Questi cominciano la transazione e cominciano a guardare se la transazione va a buon fine, fermandosi appena prima di fare il commit, dopodichè inviano al master un messaggio, dicendogli che sono pronti ad eseguire il commit (se tutto è andato bene). Raggiunta questa situazione, il master invia l'ordine alle repliche di fare commit. La transazione può fallire solo sul manager o su una delle due copie. Se si vuole mantenere una **strong consistency** e non si riceve via libera da tutte le copie, è necessario che il master invii un ordine di *rollback*. È necessario inoltre avere una certa velocità di risposta da parte delle repliche: averne anche solo una più lenta compromette la velocità del sistema.

## Server Primari e Secondari
I server seocndari non sono stati creati per aumentare le performance, ma nascono come server di copia per la disponibilità, per avere un backup.

### Backup for Disaster Recovery
Il backup sulle copie secondarie deve essere fatto periodicamente. Questo ritard di sincronizzazione può variare da qualche minuto fino a un giorno, in alcuni casi anche di più. Il server secondario viene utilizzato solo quando serve, come copia di backup. È necessario mantenere la copia secondaria a una distanza minima di 30Km (distanza studiata in previsione di disastri catastrofici che possono riguardare anche più edifici adiacenti). In generale, anche una distanza superiore a questa è considerabile una soluzione migliore. 

Il master primary ha procolli **strong** con le altre copie primary, mentre ha protocolli **weak** con le copie secondary. Se il backup è sufficientemente recente le operazioni in lettura possono essere fatte su di esso (copie secondarie), mentre le operazioni in scrittura sono sempre fatte sul principale. I protocolli di weak consistency sono anhe chiamati ***eventual consistency***, poiché la consistenza *viene garantita prima o poi*.

Il delta temporale tra gli eventi non è importante, bisogna solo ottenere un ordinamento logico. I DB non relazionali spezzano i propri dati in shardes che sono distribuiti tra i diversi server per costituirne una copia. Il massimo che garantisce un DB NoSQL è l'***eventual consistency***. 

L'idea generale è quella di utilizzare un **server intermediario**, generalemnte server che fanno da secondary (non completo, ma esclusivamente per una frazione del dato).

