[TOC]

# 2.1 Gestione dei Processi (e Processori)

Questo *gestore* normalmente prende il nome di `KERNEL` o `NUCLEO`. Nel caso in cui la strutturazione sia monolitica tutto collassa nel *nucleo* (che coincide con il sistema operativo). Questa *macchina astratta* che comprende l'hardware a disposizione deve mettere in campo:

- un *meccanismo di esecuzione di **processi sequenziali*** 
- uno o più *meccanismi di sincronizzazione*

Questa macchina (kernel o nucleo) può essere anche realizzata:

- totalmente in *hardware* (in genere non è così)
- *software* (una parte consistente) 
- *firmware*

Si analizzano di seguito i concetti essenziali del nucleo per prima cosa dal ***punto di vista ESTERNO*** (utente programmatore).

## 2.1.1 Algoritmo, Programma, Processo

`ALGORITMO` &rarr; procedimento logico che deve essere seguito per risolvere il problema in esame

`PROGRAMMA` &rarr; descrizione dell'algoritmo tramite un opportuno formalismo (*linguaggio di programmazione*), che rende possibile l'esecuzione dell'algoritmo da parte di un particolare elaboratore. Parlando di un linguaggio di programmazione, si dovrà passare per le fasi opportune, quali compilazione, collegamento (linking) fino all'ottenimento del formato eseguibile, partendo dal programma scritto in forma sorgente. Se il linguaggio è interpretato come linguaggio shell, mandiamo direttamente in esecuzione il programma senza tutte le fasi necessarie in un linguaggio di programmazione compilato.

Un programma *descrive* un *insieme* (possibilmente infinito) *di processi*, ognuno dei quali è relativo all’esecuzione del programma da parte del processore per un determinato insieme di dati in ingresso. *Ogni volta che si richiede l'esecuzione di un programma si genera un nuovo processo*.

`PROCESSO` (*sequenziale*) &rarr; sequenza di eventi a cui dà luogo un elaboratore quando opera sotto il controllo di un particolare programma.

### Differenza fra Programma e Processo

Un programma (entità statica, prima memorizzato come file nel file system e poi caricato in memoria centrale) può essere eseguito da più processi (entità dinamiche).

<center><b><i>Un processo è un programma in esecuzione</i></b></center>

## 2.1.2 Rappresentazione di un Processo

Sequenza di ***stati*** attraverso i quali passa l’elaboratore durante l’esecuzione di un programma.

> **Esempio**:
> Massimo Comune Divisore (M.C.D.) di x e y (numeri naturali)
>
> ```c
> a = x; 
> b = y;
> while (a != b) { 
>     if (a > b)
>     	a = a - b;
>     else b = b - a;
> }
> ```
>
> | **x**     | 18   | 18   | 18   | 18   | 18   | 18   |
> | ----- | ---- | ---- | ---- | ---- | ---- | ---- |
> | **y** | 24   | 24   | 24   | 24   | 24   | 24   |
> | **a** | -    | 18   | 18   | 18   | 12   | 6    |
> | **b** | -    | -    | 24   | 6    | 6    | 6    |
>
> ***a = 6***, ***b = 6*** &rarr; M.C.D dei due numeri dati in ingresso: 18, 24
>
> ***STATO*** &rarr; espresso dai valori delle variabili

La rappresentazione tabellare non è particolarmente comoda, perché se si dovesse rappresentare un programma più complesso questa diventerebbe molto grande e di difficile interpretazione. A questa, vengono preferite altre modalità di rappresentazione.

### Grafo di Precedenza

Un processo può essere rappresentato tramite un *grafo orientato* &rarr; *grafo di precedenza* del processo.

I *nodi* del grafo rappresentano i singoli eventi del processo, mentre gli *archi* identificano le *precedenze* *temporali tra tali eventi*. 

#### Processo Sequenziale

Il grafo di precedenza è a *ordinamento totale* cioè ogni nodo (fatta eccezione per quello iniziale e quello finale) *ha esattamente un predecessore ed un successore*.

> **Esempio**: (riprende *Esempio M.C.D*)
>
> <center><img src="res/02_1_grafo_orientato.PNG" alt="grafo-orientato" style="zoom: 50%;" /> </center>

Per vedere che questa non è l'unica possibilità in cui si sviluppa un programma che porti ad ottenere un grafo di precedenza a ordinamento totale e quindi a un processo sequenziale, passiamo a considerare un *formalismo matematico*.

> **Esempio**:
>
> Il programma corrisponde alla valutazione dell'espressione:
> $$
> (3 * 4) + (2 + 3) * (6 - 2)
> $$
> **GRAFO DI PRECEDENZA AD ORDINAMENTO TOTALE** 
>
> <center><img src="res/02_2_grafo_sequenziale_ordinamento_totale.PNG" alt="sequenziale-ordinamento-totale" style="zoom:50%;" /></center>
>
> **GRAFO DI PRECEDENZA AD ORDINAMENTO PARZIALE**
>
> Nel *caso in cui si è in di più* nella risoluzione dell'espressione. Con una suddivisione dei compiti, l'espressione può essere valutata svolgendo in parallelo i calcoli. Prima di calcolare D deve essere posto il *vincolo* di aver prima calcolato B e C. Con l'ordinamento parziale, *un nodo può avere più di un predecessore* o *un successore*.
>
> <center><img src="res/02_3_grafo_sequenziale_ordinamento_parziale.PNG" alt="sequenziale-ordinamento-parziale" style="zoom:67%;" /></center>
>
> ARCO &rarr; vincolo di precedenza tra gli eventi
>
> VINCOLO DI SINCRONIZZAZIONE &rarr; ordinamento di eventi

#### Processo non Sequenziale

L'***ordinamento totale*** di un grafo di precedenza *può derivare* dalla natura sequenziale di un processo, cioè può essere implicito nel problema da risolvere (il calcolo dell'MCD è possibile ottenerlo solo con un grafo a ordinamento totale). Altre volte, invece, l’ordinamento totale può essere imposto dalla natura sequenziale dell’elaboratore e non essere insito nel problema (come nel caso del formalismo matematico precedente, dove la natura del programma è tale per cui potrei ottenere un grafo di precedenza a ordinamento parziale, e quindi un *processo non sequenziale*).

Esistono molti esempi di applicazioni che potrebbero essere più naturalmente rappresentate da processi *non sequenziali* &rarr; cioè processi tra i cui eventi non esiste un ordinamento totale, ma solo *parziale*.

> **Esempio**:
>
> Elaborazione di dati su un file sequenziale (composto da N record).
>
> <center><img src="res/02_4_elaborazione_dati_file_sequenziale.PNG" alt="elaborazione-dati-file-sequenziale" style="zoom: 80%;" /></center>
>
> ```c
> T buffer;
> int i;
> 
> for (i = 1; i <= N; i++) { 
>     lettura(buffer);
>     elaborazione(buffer);
>     scrittura(buffer); 
> }
> ```
>
> Scrivendo il programma in questi termini, senza ragionare attraverso compilatori paralleli, quello che otteniamo è un'esecuzione che da luogo a un processo sequenziale e quindi a un *grafo di precedenza a ordinamento totale*.
>
> **GRAFO DI PRECEDENZA AD ORDINAMENTO TOTALE** 
>
> - **L** &rarr; lettura
> - **E** &rarr; elaborazione
> - **S** &rarr; scrittura
>
> <center><img src="res/02_5_grafo_non_sequenziale_ordinamento_totale.PNG" alt="non-sequenziale-ordinamento-totale" style="zoom: 80%;" /> </center>
>
> **GRAFO DI PRECEDENZA AD ORDINAMENTO PARZIALE**
>
> In realtà il problema non è necessariamente sequenziale, pertanto è anche possibile ottenere un *grafo a ordinamento parziale* in cui rimangono dei *vincoli di sequenzialità* (sulle letture, sulle elaborazioni e sulle scritture). Una volta fatto partire il tutto, è possibile arrivare alla lettura del terzo dato che può avvenire in parallelo con l'elaborazione del secondo dato e la scrittura del primo: non c'è nessuna ragione insita nella natura del problema che lo vieti.
>
> <center> <img src="res/02_6_grafo_non_sequenziale_ordinamento_paziale.PNG" alt="non-sequenziale-ordinamento-parziale" style="zoom:67%;" /> </center>

L’esecuzione di un processo non sequenziale richiede:

- un elaboratore non sequenziale
- un linguaggio di programmazione non sequenziale (oppure degli strumenti che automaticamente producono da un programma scritto in modo sequenziale dal programmatore, un programma non sequenziale)

#### Elaboratore non Sequenziale

In grado di eseguire più operazioni contemporaneamente &rarr; la *contemporaneità* può essere ***reale (A)*** o ***simulata (B)***.

> **Esempio**:
>
> In relazione all'esempio precedente,  la *lettura*, *elaborazione* e *scrittura* di dati diversi in contemporanea. E' necessaria una CPU differente per svolgere ciascuno di questi compiti.

**A) PIÙ ELABORATORI**

- architettura multiprocessore (più processori, memoria comune)
- architettura distribuita (più processori ognuno con memoria privata)

<center><img src="res/02_7_parallelismo_reale.PNG" alt="parallelismo-reale" style="zoom:50%;" /></center>

**B) UN SOLO ELABORATORE**

- architettura monoprocessore

**MULTIPROGRAMMAZIONE** &rarr; quasi parallelismo

<center><img src="res/02_8_quasi_parallelismo.PNG" alt="quasi-parallelismo" style="zoom:50%;" /></center>

Si tratta di un sistema di elaborazione in grado di eseguire processi non sequenziali. Deve fornire:

- una serie di ***unità di elaborazione*** (*fisiche*, quindi distinte l'una dall'altra, o *virtuali*) 
- ***meccanismi di sincronizzazione*** per imporre i vincoli di precedenza e per consentire l’interazione tra i processi. Il processo di elaborazione deve ricevere il dato su cui deve operare.

##### Linguaggi di Programmazione non Sequenziale

| <center> Linguaggi CONCORRENTI </center> | <center> Linguaggi SEQUENZIALI <br />+ Funzioni di Libreria (Primitive) </center> |
| ---------------------------------------- | ------------------------------------------------------------ |
| Concurrent Pascal (*process*)            | C per UNIX (*fork()*, ecc...)                                |
| ADA (*task*)                             |                                                              |
| Java (*thread*)                          |                                                              |

**Linguaggi Concorrenti**:

Hanno delle parole chiave nei linguaggi che consentono di definire i vari processi non sequenziali (indicati tra parentesi nella tabella).

**Linguaggi Sequenziali**:

Utilizzando un linguaggio sequenziale su dei sistemi, vengono utilizzate delle *funzioni di libreria specifiche* che si interfacciano con le opportune *system call* (primitive) del Sistema Operativo, le quali consentono di creare dei processi (ad esempio con la parola chiave *fork*).

Per superare le difficoltà dovute alla complessità di un algoritmo non sequenziale &rarr; ***scomposizione*** di un processo non sequenziale in un ***insieme di processi sequenziali*** eseguiti ”contemporaneamente” (contemporaneità reale o simulata)

Ogni processo sequenziale costituente (cioè derivato da tale scomposizione) è *analizzato e programmato separatamente*.

> **Esempio**:
>
> - *processo* di lettura (lettura del secondo dato garantita solo al completamento della lettura del primo)
>
> - *processo* di elaborazione
>
> - *processo* di scrittura

I processi possono essere eseguiti ***contemporaneamente***, ma devono rispettare i ***vincoli di precedenza*** che esistono tra le operazioni dei vari processi &rarr; **SINCRONIZZAZIONE**

`PROCESSI CONCORRENTI INTERAGENTI` &rarr; processi sequenziali di cui è *costituito* un *processo non sequenziale*.

#### Processi Concorrenti

**Definizione**: Più processi (sequenziali) si dicono ***concorrenti*** se la loro esecuzione si sovrappone nel tempo.

Due processi sono ***concorrenti*** se la ***prima*** operazione di uno inizia prima *dell’ultima dell’altro*. Indichiamo con *t1* l'istante di tempo in cui *finisce* il processo *P1*, mentre *t0* l'istante di tempo in cui *comincia* il processo *P2*. Utilizzando questa definizione non c'è bisogno di chiarire se si tratta di una sovrapposizione reale o simulata.

A) **OVERLAPPING** &rarr; *PARALLELISMO REALE* (architettura multiprocessore)

<center><img src="res/02_9_overlapping.PNG" alt="overlapping" style="zoom: 50%;" /></center>

​	P1 e P2 sono eseguiti su due CPU distinte.

B) **INTERLEAVING** &rarr; *QUASI PARALLELISMO* (architettura monoprocessore)

<center><img src="res/02_10_interleaving.PNG" alt="interleaving" style="zoom:50%;" /></center>

Utilizzando una sola CPU, potrebbe accadere che ad un certo punto P1 richieda un'operazione di I/O, liberi la CPU che viene utilizzata per l'esecuzione di P2, a ciò non è garantito. Se P1 fosse un processo *CPU bound* (si limita a fare i calcoli, senza bisogno di fare interazioni I/O), una volta che prende possesso della CPU non la lascia. In questo caso *deve intervenire il Sistema Operativo* per garantire che anche P2 possa essere eseguito.

Nel caso di un sistema **MULTIPROGRAMMATO**, risulta comune che il processore esegua sequenze di operazioni appartenenti a processi diversi (che quindi determinano *multiprocessing*). Quindi, nel Sistema Operativo sono presenti ***contemporaneamente*** più processi attivi di cui:
- uno solo è in esecuzione
- gli altri sono: 
  - ***PRONTI*** &rarr; in attesa del processore 
  - ***IN ATTESA*** di eventi (ad esempio, completamento di operazioni di I/O)

##### Evoluzione Temporale di Due Processi Concorrenti

<center><img src="res/02_11_evoluzione_temporale_processi_concorrenti.PNG" alt="evoluzione-temporale-processi-concorrenti" style="zoom:67%;" /></center>

Si fa riferimento agli istanti temporali rappresentanti l'evoluzione dei due processi concorrenti. Nell'istante *t5* entrambi i processi sono in blocco e la ***CPU*** risulta ***libera***. Queste situazioni sono possibili, ma si cerca di limitarle in modo di dare alla CPU sempre un compito da eseguire.

## 2.1.3 Stati di un Processo

- **IN ESECUZIONE** (*RUNNING*) &rarr; sta usando la CPU
- **SOSPESO** (*SLEEPING*) &rarr; in attesa di un evento
- **PRONTO** (*READY*) &rarr; pronto per l'esecuzione

<center><img src="res/02_12_stati_processo.PNG" alt="stati processo" style="zoom:67%;" /></center>

Nel caso in cui un processo fosse di tipo ***CPU bound*** (non abbandona la CPU autonomamente) può essere gestito da un Sistema Operativo ***TIME-SHARING*** in modo da forzare la transizione di stato da *esecuzione* a *pronto* andando a sottrarre la CPU, così che questa non venga monopolizzata da un unico processo. I sistemi *time-sharing* **assegnano *un quanto di tempo specifico*** per l'esecuzione di una parte del processo, una volta esaurito questo quanto *riportano il processo in stato di pronto*, e solo a un giro successivo questi possono tornare ad essere in esecuzione.

## 2.1.4 Descrittore di un Processo

Siccome quando un processo è in esecuzione utilizza la CPU (*i registri macchina della CPU*), bisogna fare in modo che *quando questo non occupa più la CPU*, ***tutte le informazioni relative allo stato di esecuzione*** vengano ***salvate***. Se non si prende questo accorgimento, il processo *non può riprendere la sua esecuzione* dalla situazione in cui era arrivato.

A questo scopo *ogni processo* possiede un ***DESCRITTORE di PROCESSO*** (*Process Control Block, PCB*) che *mantiene tutte queste informazioni* (e altre) *quando il processo non è in esecuzione* (pronto o sospeso). Questi descrittori sono solitamente *in code*, pertanto si fa riferimento alle *code di processi pronti* e alle *code di processi sospesi*. Di solito presentano almeno un *riferimento al prossimo processo in coda*, potendone presentare uno *anche* per il *processo precedente*.

# 2.2 Relazione tra Processi Concorrenti

## 2.2.1 Processi Concorrenti Disgiunti 

`PROCESSO INDIPENDENTE` &rarr; un processo che *non può influenzare o essere influenzato da altri* processi.

### Proprietà

- il suo stato *non è condiviso* da altri processi
- la sua esecuzione è *deterministica*: il *risultato della esecuzione dipende solo dai dati di ingresso*
- la sua esecuzione è *riproducibile*: il *risultato della esecuzione è sempre lo stesso* a parità dei dati in ingresso
- la sua *esecuzione* *può essere bloccata e fatta ripartire* senza provocare danni

## 2.2.2 Processi Concorrenti Interagenti

`PROCESSO INTERAGENTE` &rarr; un processo che *può influenzare o essere influenzato da altri processi*.

### Proprietà

- il suo *stato è condiviso* da altri processi
- la sua esecuzione è *non deterministica*: il *risultato della esecuzione* dipende dalla sequenza di esecuzione relativa e *non è predicibile*
- la sua esecuzione è *non riproducibile*: il *risultato della esecuzione non è sempre lo stesso* a parità dei dati in ingresso
- la sua *esecuzione* *non può essere bloccata e fatta ripartire* senza provocare danni

### Interazione tra Processi Interagenti

Si distinguono più tipi di interazioni:

- **COMPETIZIONE **&rarr; ***sincronizzazione*** *indiretta o implicita*
- **COOPERAZIONE** &rarr; ***sincronizzazione*** *diretta o esplicita*

A queste se ne aggiunge un terzo tipo, che però risulta ***indesiderata***:

- **INTERFERENZA** 

#### Competizione (indiretta)

I processi *competono per l’uso di risorse comuni*. Un problema inerentemente non sequenziale può essere risolto da un insieme di processi concorrenti che interagiscono in modo competitivo.

Il problema di base della competizione è quello della ***mutua esclusione***, cioè la *competizione per l’uso di risorse comuni che non possono essere usate contemporaneamente*. A questo problema tuttavia possono aggiungersene altri. Questo tipo di interazione introduce dei ***vincoli di sincronizzazione*** *tra gli eventi* dei processi concorrenti. Questi vincoli servono per garantire una corretta interazione.

Anche i processi indipendenti competono per l’uso esclusivo di risorse comuni.

#### Cooperazione (diretta)

Un problema inerentemente non sequenziale può essere risolto da un insieme di processi concorrenti che interagiscono in modo cooperativo.

Con questo tipo di interazione è previsto uno ***SCAMBIO DI INFORMAZIONI***. Questo scambio di informazioni può semplicemente essere: 

- *invio e la ricezione di un segnale* (senza trasferimento di dati)

- *invio e ricezione di messaggi* (con trasferimento di dati) &rarr;  in questo caso la *sincronizzazione che è necessaria* (vincoli di sincronizzazione) fra i processi spesso viene detta `COMUNICAZIONE`. É possibile pertanto considerare la comunicazione come un *caso particolare di sincronizzazione*.

Anche con questo tipo di interazione, i processi concorrenti interagenti che compongono un processo non sequenziale introducono dei vincoli di ***SINCRONIZZAZIONE*** tra gli eventi dei processi concorrenti.

#### Interferenza

Questo tipo di interazione è *provocata* da ***errori di programmazione***, in particolare può essere provocata da:

- inserimento nel programma di interazioni tra processi non richieste dalla natura del problema

- erronee soluzioni a problemi di interazione (cooperazione e competizione) necessarie per il corretto funzionamento del programma

Si tratta di un'interazione *non prevedibile e non desiderata* che dipende dalla ***velocità relativa*** tra i processi. Ciò vuol dire che una certa esecuzione in un lasso di tempo potrebbe non portare a problemi di interferenza, ma a parità dei dati di ingresso un'ulteriore esecuzione potrebbe portare al verificarsi di problemi di interferenza. Questi ***errori*** sono ***dipendenti dal tempo***, e prendono anche il nome di *corse critiche* (race condition).

## 2.2.3 Sincronizzazione

Per far modo che non si verifichino errori dipendenti dal tempo, viene introdotto un ***vincolo di sincronizzazione***. La sincronizzazione rappresenta un ***vincolo sull’ordine con cui sono eseguite le operazioni*** sui processi. Questi vincoli sono diversi a seconda dell'interazione:

- Nel caso di *INTERAZIONE INDIRETTA*, il vincolo deve rendere *impossibile* l’esecuzione contemporanea delle operazioni con le quali i *processi accedono a risorse comuni* (caso della mutua esclusione).

- Nel caso di *INTERAZIONE DIRETTA*, il vincolo *deve imporre* che le operazioni di un processo *non abbiano inizio prima o dopo determinate operazioni di altri processi*.

In ogni caso sono necessari **MECCANISMI DI SINCRONIZZAZIONE** &rarr; non usare sincronizzazione o sbagliarla implica introdurre degli *ERRORI DIPENDENTI DAL TEMPO*.

> **Esempio**:
>
> Si riprende in considerazione l'esempio di elaborazione di dati su un file sequenziale (in particolare, il *grafo ad ordinamento parziale*). 
>
> In questo caso il processo E (elaboratore), non può svolgere le operazioni di elaborazione sul secondo dato prima che il processo L (lettore) non abbia letto il secondo dato e glielo abbia comunicato. La *freccia* tra i due eventi rappresenta un *vincolo di sincronizzazione diretta*. Il processo lettore dovrà avere un modo per comunicare il secondo dato che è stato letto al processo di elaborazione. Così come il processo di elaborazione dovrà fornirlo al processo scrittore (mediante una comunicazione). Le frecce verticali rappresentano la *normale sequenzialità delle operazioni del singolo processo*. Un qualsiasi processo L (così come gli altri) non può leggere il dato *i* se prima non ha letto il dato *i-1* e così via. Fra i vari processi allo stesso modo esistono delle relazioni di sincronizzazione (comunicazione) per cui il processo lettore, fino a che non ha comunicato il dato letto al processo di elaborazione non può passare a leggere il successivo.

### Strumenti di Sincronizzazione

Due diversi modelli logici di riferimento:
* MODELLO AD AMBIENTE GLOBALE &rarr; la prima configurazione a livello hardware era di avere una sola CPU con una memoria comune.
* il primo definito a livello storico
* detto anche Modello a memoria comune
* MODELLO AD AMBIENTE LOCALE &rarr; è il modello dei processi utilizzato in UNIX (e quindi anche in Linux), scelto nonostante avessero anch'essi una sola CPU e una sola memoria.
  * il secondo definito a livello storico
  * detto anche Modello a scambio di messaggi
  * maggiore *PROTEZIONE* e *CORRETTEZZA* (ogni processo ha il proprio spazio di indirizzamento che non è condiviso con gli altri processi)

#### Modello ad Ambiente Locale

Ogni ***applicazione*** viene strutturata come un ***insieme di processi***, ciascuno operante in un *ambiente locale* *non accessibile direttamente da nessun altro processo*. In questo modello si può avere ***solo un tipo di interazione*** fra processi: la *COOPERAZIONE*. Ogni forma di interazione tra processi (sincronizzazione) avviene tramite *scambio di messaggi* o *invio di segnali*.

A livello logico l'applicazione è *composta esclusivamente da processi*:

<img src="res/02_13_modello_ambiente_locale.PNG" alt="modello-ambiente-locale" style="zoom: 33%;" />

#### Modello ad Ambiente Globale

Ciascuna applicazione viene strutturata come un *insieme di componenti*, suddivisa in due insiemi disgiunti:
- PROCESSI (componenti attivi)
- RISORSE (componenti passivi)

<img alt="modello-ambiente-globale" src="res/02_14_modello_ambiente_globale.PNG" style="zoom:33%;" />

`RISORSA` &rarr; qualunque oggetto, fisico (es. stampante) o logico (es. memoria) di cui un processo necessita per portare a termine il suo compito. Una risorsa è generalmente utilizzata da più processi. In questo tipo di modelli le risorse sono raggruppate in classi.

Una ***classe di risorse*** identifica l’*insieme di tutte e sole le operazioni che un processo può eseguire per operare su risorse di quella classe*.

> **Esempio**:
>
> Si ha la classe di risorse denominata "Stampante". Se a livello architetturale si hanno a disposizione 3 stampanti, le operazioni che si possono fare su *una delle specifiche istanze* della classe Stampante, sono le stesse, ma si hanno più istanze appartenenti alla classe.

Nel modello ad ambiente globale si possono avere ***due tipi di interazione*** fra processi:
*COMPETIZIONE* e *COOPERAZIONE*. Gli STRUMENTI DI SINCRONIZZAZIONE sono: semafori (meccanismi semplici, a livello di S.O), monitor (più sofisticati, a livello di programmazione), ecc...

I vari problemi di sincronizzazione nell'ambito dei processi concorrenti interagenti possono essere sia *problemi di competizione* che *problemi di cooperazione*.

### Problemi di Competizione

#### Mutua Esclusione

Si ha necessità di mutua esclusione quando ***non più di un processo alla volta*** *può accedere ad una risorsa comune* &rarr; *problema base della competizione* fra processi (modello ad ambiente globale).

> **Esempio di Risorsa Comune**:
>
> Insieme di variabili contenute in memoria centrale (comune).

<img src="res/02_15_mutua_esclusione.PNG" alt="mutua-esclusione" style="zoom: 50%;" />

Supponendo di avere un solo processore, definiamo i segmenti *P1* e *P2* l'insieme delle istruzioni che vanno a modificare le variabili comuni da parte dei rispettivi processi P1 e P2.  La ***regola*** di mutua esclusione impone che le ***operazioni*** con le quali i processi accedono alle variabili comuni ***non si sovrappongano nel tempo***. In figura vengono riportati i due scenari dove nel primo opera prima P1, mentre nel secondo opera prima P2. Non viene imposto ***nessun vincolo sull'ordine*** on il quale le operazioni sulle variabili comuni sono eseguite.

> **Esempio di Mutua Esclusione (1)**:
>
> P1 e P2 accedono a due variabili comuni: 
>
> - un contatore (*cont*) che deve incrementare ogniqualvolta si effettua una determinata azione 
> - un identificatore (*id*) che deve tenere traccia dell’ultimo processo che ha effettuato l’azione 
>
> Al completamento dell’esecuzione dei processi, *cont* deve contenere un valore pari al numero complessivo delle azioni effettuate dai due processi e *id* deve contenere l’identificatore dell’ultimo processo che ha effettuato l’azione. Si consideri che un processo può eseguire un frammento di codice anche più volte (inserendolo ad esempio in un ciclo for).
>
> <img src="res/02_16_mutua_esclusione_primo_esempio.PNG" alt="mutua-esclusione-primo-esempio" style="zoom: 67%;" />
>
> Con la sintassi <azione> si fa riferimento a una parte di codice eseguita da ciascun processo.
>
> **Possibile sequenza di esecuzione**:
>
> > t0:   <azione> 			   (P1)
> > t1:   <azione> 			   (P2)
> > t2:   cont = cont + 1; 	  (P2)
> > t3:   id = P2; 					(P2)
> > t4:   cont = cont + 1; 	  (P1)
> > t5:   id = P1; 					(P1)
>
> A livello pratico, il contatore è stato incrementato di *2 unità*, e il codice è stato eseguito *2 volte*, prima da P1, poi da P2. Per quanto riguarda l'ID, il valore non è corretto in quanto risulta che l'ultimo processo a eseguire l'azione è stato P1, ma è chiaramente visibile dalla sequenza che in realtà l'ultimo ad averlo eseguito è stato P2.

> **Esempio di Mutua Esclusione (2):**
>
> Due processi (P1 e P2) hanno accesso ad una struttura dati organizzata a pila (STACK) per inserire e prelevare informazioni (sempre dalla prima posizione, il top dello stack).
>
> <img src="res/02_17_struttura_dati_stack.PNG" alt="struttura-dati-stack" style="zoom:67%;" />
>
> I due processi fanno uso delle due procedure:
>
> - INSERIMENTO
> - PRELIEVO
>
> <img src="res/02_18_mutua_esclusione_secondo_esempio.PNG" alt="mutua-esclusione-secondo-esempio" style="zoom:67%;" />
>
> L’esecuzione contemporanea delle due procedure può portare ad un uso scorretto della risorsa STACK.
>
> **Possibile sequenza di esecuzione**: (peggior caso possibile, caso di *corsa critica*)
>
> > t0:   top = top + 1 		(P1)
> > t1:   x = stack[top] 	   (P2)
> > t2:   top = top - 1 		 (P2)
> > t3:   stack[top] = y 	   (P1)
>
> Quando nell'istante *t1* si fa un'operazione di *prelievo* puntando al top, il top risulta ancora vuoto, in quanto alla corrispettiva posizione dell'array non è ancora stata assegnata il valore y, pertanto la precedente operazione di inserimento non risulta completata.

##### Sezione Critica

Una sezione critica è la ***sequenza di operazioni*** *con le quali un processo accede e modifica un insieme di variabili comuni*.

Ad un insieme di variabili comuni possono essere associate:

- UNA SOLA sezione critica (usata da tutti i processi)
- PIÙ sezioni critiche (classe di sezioni critiche) &rarr; nello scenario più completo la classe potrebbe essere degenere e contenere una sola sezione critica.

Per capire meglio questi termini, riprendiamo gli esempi precedenti:

- Nel primo esempio, i processi usavano variabili comuni (cont e id) tramite una sola sezione critica (*cont = cont+1; id = getpid();*)

+ Nel secondo esempio, i processi usavano variabili comuni (top e stack) tramite due sezioni critiche, che quindi definivano una classe di sezioni critiche (INSERIMENTO e PRELIEVO).

La *REGOLA di MUTUA ESCLUSIONE* stabilisce che: ***sezioni critiche appartenenti alla stessa classe devono escludersi mutuamente nel tempo***. In altre parole, *una sola sezione critica di una classe può essere in esecuzione ad ogni istante* di tempo.

## 2.2.4 Soluzioni 

Come soluzione al problema della mutua esclusione si devono utilizzare degli ***STRUMENTI DI SINCRONIZZAZIONE***. La sezione critica *deve essere preceduta* da una parte di ***PROLOGO*** e *seguita* da una parte di ***EPILOGO***.

- PROLOGO &rarr; deve assicurarsi che il processo sia l'unico che sta cercando di eseguire la porzione critica. Deve verificare la disponibilità della sezione critica, e in caso affermativo, acquisirla.
- EPILOGO &rarr; fa in modo che sia chiaro al sistema operativo che il processo non è più in esecuzione sulla sezione critica. Si deve occupare del rilascio della sezione critica (in modo che altri processi possano eseguirla)

> ...
>
> PROLOGO
>
> SEZIONE CRITICA
>
> EPILOGO
>
> ...

Nel seguito considereremo (se non diversamente specificato) il CASO PARTICOLARE di avere due processi P1 e P2.

### Requisiti per una Soluzione Accettabile

1. Mutua esclusione dei processi che eseguono le sezioni critiche

2. Indipendenza dei processi che eseguono le sezioni critiche (l'esecuzione di un processo non deve essere un prerequisito per l'esecuzione di un altro).

3. Assenza di condizioni di stallo (deadlock a livello di strumento di sincronizzazione) per i processi che eseguono le sezioni critiche

   `DEADLOCK` &rarr; i processi sono bloccati nell'attesa di verificarsi di situazioni che non si possono verificare

4. Assenza di attese attive per i processi che eseguono le sezioni critiche (conviene mettere in attesa un processo se questo non può eseguire la sezione critica)

5. Assenza di *starvation* per i processi che eseguono le sezioni critiche (i processi messi in attesa devono essere riattivati non appena possibile.

   `STARVATION` &rarr; dimenticarsi di un processo nella coda dei processi pronti

6. Le sezioni critiche eseguite con interruzioni abilitate

**Osservazione**: Si suppone che ogni processo sia eseguito ad una velocità diversa da 0  &rarr; non si può fare alcuna ipotesi sulla velocità relativa dei processi. Le sezioni critiche non hanno quindi una velocità istantanea di esecuzione.

I requisiti **1**, **2** e **3** sono di ***CARATTERE LOGICO*** e ’implementazione del meccanismo di mutua esclusione è ***corretto*** solo se presenta questi tre requisiti. Il requisito **5** è di ***ordine realizzativo*** ed è ancora legato alla ***CORRETTEZZA*** dell’implementazione. 

I requisiti **4** e **6** sono di ordine realizzativo e non riguardano la correttezza del meccanismo, ma l’***EFFICIENZA*** della sua implementazione. 

Il problema della mutua esclusione può, in prima approssimazione, essere risolto rispondendo solo ai REQUISITI 1, 2, 3, e 5, trascurando i requisiti 4 e 6 ***&rarr; IPOTESI DI SEZIONI CRITICHE BREVI***

### Sezioni Critiche Sufficientemente Brevi

Posso avere una soluzione al problema della mutua esclusione introducendo delle *attese attive*. Con questa soluzione il sistema non è efficiente, in quanto si sprecano dei cicli di clock per nulla, ma potrebbe comunque rappresentare una soluzione per sezioni critiche sufficientemente brevi. In alternativa, posso decidere di rappresentare lo strumento di sincronizzazione attraverso la *disabilitazione delle interruzioni*, perché ciò, nel caso di sistemi monoprocessore, realizza immediatamente la mutua esclusione. Disabilitando le interruzioni non può né intervenire un processo perché lo scheduler ha scoperto che è scaduto il quanto di tempo (che rappresenta un'interruzione) né si può accorgere che è intervenuto un processo a maggiore priorità, in quanto questo nuovo evento (l'avere un nuovo processo nella coda dei processi pronti) non viene evidenziato.

Sia {A, B, C, ...} una classe di sezioni critiche ”sufficientemente” brevi

#### Soluzione in Caso Monoprocessore

PROLOGO &rarr; DISABILITAZIONE INTERRUZIONI
	violazione requisito n. 6
EPILOGO &rarr; ABILITAZIONE INTERRUZIONI

#### Soluzione in Caso Multiprocessore

USO DI OPERAZIONI INDIVISIBILI (***PRIMITIVE***) chiamate 
PROLOGO &rarr; LOCK (X)
	violazione requisito n. 4
EPILOGO &rarr; UNLOCK (X)

```c
typedef enum {false, true} Boolean;
void LOCK (Boolean X) {
	while (X);			// fino a che X è true si rimane in attesa attiva
	X = true
}
void UNLOCK (Boolean X) {
    X = false;
}
```

**X** è un ***indicatore*** associato alla classe di sezioni critiche (un indicatore per ciascuna classe):

- **X == false** nessuna sezione critica in esecuzione &rarr; il processo che trova X a false può metterlo a true e cominciare ad eseguire
- **X == true** una sezione critica in esecuzione &rarr; un processo sta eseguendo una funzione critica quindi ogni altro processo che tenta di fare l'epilogo per poi passare ad eseguire la sezione critica viene bloccato

`INDIVISIBILITA'` &rarr; nessun altro processo può andare ad eseguire la LOCK se questa è in esecuzione sulla stessa variabile X da parte di un altro processo. 

Per quanto riguarda l'*UNLOCK* in un singolo ciclo di clock viene posto *X = false*, nello stesso ciclo di clock nessun altro può avere accesso alla memoria centrale e andare a interrompere l'assegnamento. La *LOCK* è più problematica, in quanto presenta una verifica del valore di X e dall'assegnazione *X = true* quando questa ha valore false.

L’**INDIVISIBILITÀ** della LOCK (in particolare, per le due funzioni di LOCK e UNLOCK) viene *garantita da istruzioni hardware* :

- TEST-AND-SET
- EXCHANGE

##### LOCK con TEST-AND-SET

Questa istruzione HARDWARE, ***in un solo ciclo di clock***, verifica e assegna valore alla variabile su cui agisce. A livello logico (astratto):

```c
typedef enum {false, true} Boolean;
Boolean TEST_AND_SET (Boolean &parametro) { 
    Boolean valore = parametro;
	parametro = true;
	return valore;
}
```

L’operazione LOCK diviene:

```c
void LOCK (Boolean X) {
	while (TEST_AND_SET(X));
}
```

La *TEST-AND-SET* verifica il valore e lo setta contestualmente a *true*.

- se *X == false* torna il valore precedente ma avendo contestualmente cambiato il valore di X a true, facendo terminare il ciclo while, terminando a sua volta la LOCK e quindi l'EPILOGO. Terminato l'epilogo, il processo può andare ad eseguire la *sezione critica* della classe protetta dal booleano X. Una volta eseguita questa parte dovrà essere eseguita l'UNLOCK affinché riporti il valore a false.
- se *X == true* (è già presente un processo che sta eseguendo la sezione critica della classe X) la funzione setta nuovamente X a true e ritorna true, andando a bloccare in un ciclo di attesa attiva il processo mediante un *while(true)*. Questo rimarrà tale fino a quando il processo che sta eseguendo la sezione critica della classe associata a X andrà ad eseguire l'UNLOCK, ponendo X = false.

Non c'è nessuna garanzia che il primo processo che tenta di accedere alla sezione critica della classe occupata sia il primo ad accedere alla sezione critica. 

##### LOCK con EXCHANGE

Questa istruzione hardware, in un solo ciclo di clock, scambia il valore di due variabili. 

```c
typedef enum {false, true} Boolean;
void EXCHANGE (Boolean &a, Boolean &b); { 
    Boolean temp = a;
	a = b;
	b = temp;
}
```

La funzione *EXCHANGE* si limita a scambiare il valore di due variabili passate per riferimento.

La funzione LOCK diviene:

```c
void LOCK (Boolean X) { 
    Boolean priv;
	priv = true;
	do
        EXCHANGE(X, priv);
	while (priv == true);
}
```

la funzione LOCK deve definire una *variabile privata*, ovvero allocata sullo *stack della LOCK*, ovvero sullo *stack privato del processo*, non è una variabile condivisa come la X. In questo modo ciascun processo che esegue la LOCK ha una propria istanza della variabile *priv*. Nel ciclo di attesa attivo si va a scambiare il valore di *priv* (inizializzato a true) con X:

- se *X == false*, *X* assume il valore true, mentre *priv* assume il valore di false. In questo caso la LOCK viene definita *PASSANTE*. L'EPILOGO ha successo e quindi il processo può eseguire la sezione critica associata alla classe di sezione critica X e contestualmente la X viene mesa a false, quindi vengono bloccati eventuali tentativi di accesso con la LOCK.
- se *X == true*, lo scambio è inutile ma viene comunque fatto, ponendo *X* a true e *priv* a true. Poiché priv è ora uguale a true si rimane nel ciclo di attesa attiva.

### Semaforo

Un semaforo S rappresenta una ***istanza*** di un ***tipo di dato astratto*** (*Semaphore*). 

`TIPO DI DATO ASTRATTO` &rarr; simile al concetto di *classe* per i linguaggi di programmazione a oggetti (come ad esempio Java). In esso è però *assente il concetto di ereditarietà*.

Un Semaforo presenta la *rappresentazione interna di ogni sua istanza* (oggetto, in termini di linguaggi di programmazione) mediante un *valore intero* (valore del semaforo) e una *coda di descrittori dei processi*, i quali saranno accodati all'interno dei semafori in attesa delle condizioni necessarie per poter proseguire nella loro esecuzione. Un Semaforo *non implica un'attesa attiva*. Il processo all'interno della sua coda si sospende qualora le condizioni per la sua esecuzione non risultano soddisfatte. Un Semaforo presenta inoltre due operazioni indivisibili chiamate *wait(S)* e *signal(S)*, ciascuna che prende come parametro una certa istanza di semaforo.

La sua struttura si presenta dunque in questo modo:

> - DATO
>   	valore intero
>   	coda descrittori
>
> - OPERAZIONI (INDIVISIBILI)
>            wait (S)
>            signal (S)

Le due funzioni possono anche essere chiamate diversamente:

- WAIT (S) &rarr; P(S) (si effettua una verifica che presenta un *if*)
- SIGNAL (S) &rarr; V(S)

Ad un Semaforo S è associato un valore intero non negativo *Sv*, con valore iniziale: 
$$
S_0 ≥ 0
$$
La *coda di attesa* associata ad un generico Semaforo S verrà riconosciuta con il simbolo Q<sub>S</sub> dove S è l'istanza specifica del semaforo. Essendo le due operazioni precedentemente definite *INDIVISIBILI*, mentre un processo esegue un'operazione non deve essere possibile per nessun altro processo eseguire nessun'altra operazione sullo stesso semaforo.

#### Operazioni su un Semaforo

###### WAIT(S)

```c
void WAIT(Semaphore S) {
if (Sv == 0) {
	< il processo viene SOSPESO
	e il suo descrittore viene inserito in QS >
}
else Sv = Sv - 1;
}
```

- Se *Sv == 0* (semaforo rosso), la WAIT viene detta SOSPENSIVA, in quanto non ha avuto successo e il processo è stato sospeso. Non può essere eseguita nessun'altra istruzione seguente all'invocazione della WAIT perché il processo risulta bloccato.
- Se *Sv ≥ 0* (condizione necessaria, un semaforo non può avere valore negativo), allora il valore viene decrementato e la WAIT ha successo. Il processo in questo modo può proseguire con l'istruzione successiva.

###### SIGNAL(S)

```c
void SIGNAL(Semaphore S) {
if (<esiste un processo P nella coda QS?>) {
    < il suo descrittore viene tolto da QS e
    lo stato di P modificato in PRONTO >
}
else Sv = Sv + 1;
}
```

- Se *Q<sub>S</sub> presenta processi in coda* viene estratto il primo processo nella coda (FIFO) e inserito nella *coda dei processi pronti*. La coda *Q<sub>S</sub>* rappresenta la *coda dei processi sospesi, mentre la Signal cambia lo stato di questi processi, togliendoli della coda e *segnandoli come pronti*. Questa operazione *non può mettere in esecuzione*, poiché in esecuzione c'è chi sta eseguendo la Signal.

- Se *Q<sub>S</sub> non presenta processi in coda* il valore del semaforo viene incrementato.

#### Invariante dei Semafori

$$
Sv = S0 + ns(S) - nw(S)
$$

- **Sv** &rarr; valore del semaforo
- **S<sub>0</sub>** &rarr; valore iniziale del semaforo
- **ns(S)** &rarr; numero di volte che è stata eseguita la signal con nessun processo in coda
- **nw(S)** &rarr; numero di volte che è stata eseguita la wait con successo

Dato che
$$
Sv ≥ 0
$$
 allora
$$
nw(S) ≤ ns(S) + S0
$$
Questa relazione è ***INVARIANTE*** cioè *è sempre vera* qualunque sia il numero di primitive (WAIT e SIGNAL) eseguite su un qualunque semaforo.

#### Utilizzo di un Semaforo per Garantire la Mutua Esclusione

I Semafori rappresentano un ***meccanismo*** per sospendere e riattivare i processi. Questo meccanismo può essere usato per realizzare ad esempio una **politica** di mutua esclusione.

```c
Semaphore MUTEX;
/* valore iniziale MUTEX_0 = 1 */
```

In questo ambito i semafori vengono normalmente chiamati *MUTEX* (*mutual exclusion*). Questi Semafori devono avere sempre come valore iniziale 1.

<img src="res/02_19_semaforo_mutua_esclusione.PNG" alt="semaforo-mutua-esclusione" style="zoom:50%;" />

> **Esempio:** Possibile Sequenza di Operazioni di Sincronizzazione
>
> <img src="res\02_20_semaforo_mutua_esclusione_primo_esempio.PNG" alt="semaforo-mutua-esclusione-primo-esempio" style="zoom: 67%;" />
>
> <img src="res/02_21_semaforo_mutua_esclusione_secondo_esempio.PNG" alt="semaforo-mutua-esclusione-secondo-esempio" style="zoom: 60%;" />
>
> **Nota**: Il valore del semaforo è rimasto a 0 perché P1 è dentro la sezione critica. Se arrivasse un processo P3, o lo stesso P2 fosse in un ciclo per tornare dentro la sezione critica questo processo deve essere bloccato *fuori dalla sezione critica*.
>
> <img src="res/02_22_semaforo_mutua_esclusione_terzo_esempio.PNG" alt="semaforo-mutua-esclusione-terzo-esempio" style="zoom: 67%;" />

#### Soddisfacimento dei Requisiti

1. Un solo processo alla volta si può trovare nella sezione critica:

$$
n = nw(MUTEX) - ns(MUTEX)
$$

<center><b><i>numero di processi entro la sezione critica</i></b></center>

**NOTA**: n ≥ 0

L’invariante
$$
nw(MUTEX) ≤ ns(MUTEX) + MUTEX_0
$$
diventa
$$
nw(MUTEX) ≤ ns(MUTEX) + 1
$$

$$
n = nw(MUTEX) - ns(MUTEX) ≤ 1
$$

Quindi
$$
0 ≤ n ≤ 1
$$

2. Un processo può bloccarsi SOLO se la sezione critica è occupata. infatti, se un processo si blocca su un semaforo è perché:

$$
MUTEXv == 0
$$

Quindi dalla relazione invariante dei semafori:
$$
MUTEXv = ns(MUTEX) - nw(MUTEX) + MUTEX0
$$
diventa
$$
nw(MUTEX) = ns(MUTEX) + 1
$$
e quindi
$$
n = nw(MUTEX) - ns(MUTEX) = 1
$$

3. Assenza di condizioni di stallo &rarr; il meccanismo del semaforo non le presenta per lo stesso motivo appena dimostrato.
4. Un semaforo non presenta attese attive.
5. Non c'è *starvation* in quanto perché la coda è gestita *FIFO*, in questo modo viene garantito che il primo processo sospeso è il primo ad essere riattivato.
6. Non si è parlato di disabilitazione delle interruzioni 